{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "202f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch as torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e09000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Project at: c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\n"
     ]
    }
   ],
   "source": [
    "# Add the project root path if not already present\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # move up one level from notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Visual confirmation\n",
    "print(\"[‚úì] Project at:\", PROJECT_ROOT)\n",
    "\n",
    "from src.data.load_data import load_train_data, EDGCDataset\n",
    "from src.data.stratified_split import stratified_split_pad_torch\n",
    "from src.models.model_trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c79eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] Loaded X_train with 6179 sequences\n",
      "[‚úì] Loaded y_train with shape (6179, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_train_data()\n",
    "\n",
    "durations = np.array([len(x) / 300 for x in X_train])\n",
    "\n",
    "cls_count = y_train[0].groupby(y_train[0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0376f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5564, 18286]) torch.Size([5564, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, lengths_train, lengths_val, y_train, y_val = stratified_split_pad_torch(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = EDGCDataset(X_train, lengths_train, y_train)\n",
    "val_dataset = EDGCDataset(X_val, lengths_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6afc8",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "\n",
    "## ECGNet Model Architecture\n",
    "\n",
    "The `ECGNet` class implements a neural network for ECG signal classification. The architecture consists of:\n",
    "\n",
    "- **Spectrogram transformation**: Converts raw ECG signals into spectrograms using Short-Time Fourier Transform (STFT).\n",
    "- **Convolutional layers**: Two sequential 2D convolutional layers with ReLU activations and max pooling, which extract spatial features from the spectrogram.\n",
    "- **Recurrent layer**: An LSTM layer processes the sequence of features, capturing temporal dependencies.\n",
    "- **Fully connected layer**: The final linear layer maps the LSTM output to the target number of classes.\n",
    "\n",
    "The model expects ECG signals and their lengths as input, and produces class logits for classification tasks.\n",
    "\n",
    "The `ECGNet` class is implemented on the module /models/model_1.py  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model_1 import ECGNet\n",
    "from src.models.hyperparamter_tunning import hyperparameter_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "274ed410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([5564, 18286])\n",
      "lengths_train: torch.Size([5564])\n",
      "y_train: torch.Size([5564, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"lengths_train: {lengths_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e649ea6",
   "metadata": {},
   "source": [
    "### üîç Hyperparameter Search and Model Selection\n",
    "\n",
    "In this section, we perform **hyperparameter tuning** to improve the performance of our machine learning models.\n",
    "\n",
    "When training neural networks, model performance is highly sensitive to hyperparameters such as the number of layers, hidden units, learning rate, dropout rate, and more. Manually selecting these values is inefficient and often suboptimal. Therefore, we use a **grid search strategy** to systematically explore combinations of hyperparameter values.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Why Hyperparameter Tuning?\n",
    "\n",
    "- Different combinations can lead to **very different results**, even with the same architecture.\n",
    "- Some configurations may **overfit** or **underfit**, while others may **generalize better**.\n",
    "- Automated tuning helps us identify the **best performing model** on the validation set without manual trial-and-error.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è How it Works\n",
    "\n",
    "1. We define a **search space**, i.e., a dictionary of hyperparameter values to test.\n",
    "2. We generate all **combinations** using Cartesian product (`itertools.product`).\n",
    "3. For each combination:\n",
    "   - Initialize the model with the current hyperparameters.\n",
    "   - Train it for a fixed number of epochs on the training set.\n",
    "   - Evaluate its performance on the validation set using **F1-score** and **loss**.\n",
    "4. Store and compare results, selecting the configuration that performs **best on validation data**.\n",
    "\n",
    "This process is repeated for each of the two model architectures used in this project:\n",
    "\n",
    "- A **CNN-LSTM hybrid** based on time-frequency STFT features.\n",
    "- A **Temporal Convolutional Network (TCN)** using a stacked 1D convolutional architecture.\n",
    "\n",
    "---\n",
    "\n",
    "By applying this strategy, we ensure our final models are both **well-tuned** and **generalizable**, which is essential for real-world performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f8f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0238 | Train F1: 0.1853 | Val Loss: 1.0021 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0173 | Train F1: 0.1853 | Val Loss: 1.0039 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0159 | Train F1: 0.1853 | Val Loss: 1.0058 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0139 | Train F1: 0.1863 | Val Loss: 1.0019 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0156 | Train F1: 0.1853 | Val Loss: 1.0137 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0143 | Train F1: 0.1856 | Val Loss: 0.9956 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0069 | Train F1: 0.1853 | Val Loss: 0.9901 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0292 | Train F1: 0.1968 | Val Loss: 1.0060 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0208 | Train F1: 0.1853 | Val Loss: 1.0056 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0173 | Train F1: 0.1853 | Val Loss: 1.0049 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0196 | Train F1: 0.1853 | Val Loss: 1.0057 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0170 | Train F1: 0.1853 | Val Loss: 1.0068 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0130 | Train F1: 0.1853 | Val Loss: 1.0091 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0152 | Train F1: 0.1853 | Val Loss: 1.0043 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0564 | Train F1: 0.1962 | Val Loss: 1.0068 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0344 | Train F1: 0.1879 | Val Loss: 1.0068 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0275 | Train F1: 0.1851 | Val Loss: 1.0242 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0233 | Train F1: 0.1879 | Val Loss: 1.0323 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0234 | Train F1: 0.1866 | Val Loss: 1.0160 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0195 | Train F1: 0.1887 | Val Loss: 1.0050 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0207 | Train F1: 0.1886 | Val Loss: 1.0024 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0198 | Train F1: 0.2021 | Val Loss: 0.9917 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9948 | Train F1: 0.1898 | Val Loss: 0.9834 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9826 | Train F1: 0.1884 | Val Loss: 0.9539 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9677 | Train F1: 0.1900 | Val Loss: 0.9151 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 0.9181 | Train F1: 0.2462 | Val Loss: 0.9171 | Val F1: 0.3000\n",
      "Epoch 6/7 | Train Loss: 0.8718 | Train F1: 0.2862 | Val Loss: 0.8005 | Val F1: 0.3100\n",
      "Epoch 7/7 | Train Loss: 0.8297 | Train F1: 0.3184 | Val Loss: 0.8373 | Val F1: 0.3090\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0307 | Train F1: 0.2070 | Val Loss: 0.9798 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0012 | Train F1: 0.1982 | Val Loss: 0.9683 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9908 | Train F1: 0.1966 | Val Loss: 0.9567 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9835 | Train F1: 0.1937 | Val Loss: 0.9596 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 0.9876 | Train F1: 0.1885 | Val Loss: 0.9477 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 0.9823 | Train F1: 0.1989 | Val Loss: 0.9238 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 0.9608 | Train F1: 0.1916 | Val Loss: 0.9504 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.1193 | Train F1: 0.2281 | Val Loss: 1.0068 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0393 | Train F1: 0.1946 | Val Loss: 1.0042 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0238 | Train F1: 0.1860 | Val Loss: 1.0008 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0233 | Train F1: 0.1902 | Val Loss: 0.9892 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0266 | Train F1: 0.1858 | Val Loss: 1.0064 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0169 | Train F1: 0.1853 | Val Loss: 0.9942 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0166 | Train F1: 0.1853 | Val Loss: 0.9983 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0291 | Train F1: 0.1943 | Val Loss: 0.9846 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9867 | Train F1: 0.1924 | Val Loss: 0.9472 | Val F1: 0.2216\n",
      "Epoch 3/7 | Train Loss: 0.9650 | Train F1: 0.2076 | Val Loss: 0.9278 | Val F1: 0.2035\n",
      "Epoch 4/7 | Train Loss: 0.9430 | Train F1: 0.2413 | Val Loss: 0.9052 | Val F1: 0.2139\n",
      "Epoch 5/7 | Train Loss: 0.9147 | Train F1: 0.2605 | Val Loss: 0.8478 | Val F1: 0.2911\n",
      "Epoch 6/7 | Train Loss: 0.8625 | Train F1: 0.2956 | Val Loss: 0.7929 | Val F1: 0.3233\n",
      "Epoch 7/7 | Train Loss: 0.8287 | Train F1: 0.3463 | Val Loss: 0.7696 | Val F1: 0.3541\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0631 | Train F1: 0.2023 | Val Loss: 0.9948 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0105 | Train F1: 0.1888 | Val Loss: 0.9841 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9904 | Train F1: 0.1882 | Val Loss: 0.9598 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9750 | Train F1: 0.1953 | Val Loss: 0.9161 | Val F1: 0.1855\n",
      "Epoch 5/7 | Train Loss: 0.9463 | Train F1: 0.2260 | Val Loss: 0.9064 | Val F1: 0.2654\n",
      "Epoch 6/7 | Train Loss: 0.9116 | Train F1: 0.2564 | Val Loss: 0.8481 | Val F1: 0.2576\n",
      "Epoch 7/7 | Train Loss: 0.8842 | Train F1: 0.2767 | Val Loss: 0.8177 | Val F1: 0.2680\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.1384 | Train F1: 0.2336 | Val Loss: 0.9958 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0442 | Train F1: 0.1979 | Val Loss: 1.0018 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0298 | Train F1: 0.1966 | Val Loss: 0.9888 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0262 | Train F1: 0.1896 | Val Loss: 0.9946 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0189 | Train F1: 0.1876 | Val Loss: 0.9893 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0161 | Train F1: 0.1893 | Val Loss: 0.9881 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0100 | Train F1: 0.1855 | Val Loss: 0.9766 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0277 | Train F1: 0.1879 | Val Loss: 1.0122 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0181 | Train F1: 0.1853 | Val Loss: 1.0087 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0209 | Train F1: 0.1938 | Val Loss: 1.0170 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0183 | Train F1: 0.1853 | Val Loss: 1.0102 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0175 | Train F1: 0.1853 | Val Loss: 1.0159 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0216 | Train F1: 0.1860 | Val Loss: 1.0046 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0158 | Train F1: 0.1853 | Val Loss: 1.0038 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0313 | Train F1: 0.1891 | Val Loss: 1.0176 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0223 | Train F1: 0.1856 | Val Loss: 1.0044 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0248 | Train F1: 0.1864 | Val Loss: 1.0164 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0208 | Train F1: 0.1852 | Val Loss: 1.0044 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0188 | Train F1: 0.1853 | Val Loss: 1.0049 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0255 | Train F1: 0.1874 | Val Loss: 1.0021 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0218 | Train F1: 0.1852 | Val Loss: 1.0142 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0562 | Train F1: 0.2017 | Val Loss: 1.0103 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0325 | Train F1: 0.1893 | Val Loss: 1.0074 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0353 | Train F1: 0.1893 | Val Loss: 1.0109 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0332 | Train F1: 0.1924 | Val Loss: 1.0094 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0336 | Train F1: 0.1904 | Val Loss: 1.0049 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0276 | Train F1: 0.1909 | Val Loss: 1.0023 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0263 | Train F1: 0.1912 | Val Loss: 1.0311 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0032 | Train F1: 0.1935 | Val Loss: 0.9711 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9779 | Train F1: 0.1859 | Val Loss: 0.9356 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9336 | Train F1: 0.2219 | Val Loss: 0.8667 | Val F1: 0.2365\n",
      "Epoch 4/7 | Train Loss: 0.8664 | Train F1: 0.2892 | Val Loss: 0.7887 | Val F1: 0.2984\n",
      "Epoch 5/7 | Train Loss: 0.7994 | Train F1: 0.3591 | Val Loss: 0.7305 | Val F1: 0.3901\n",
      "Epoch 6/7 | Train Loss: 0.7682 | Train F1: 0.4327 | Val Loss: 0.7041 | Val F1: 0.4515\n",
      "Epoch 7/7 | Train Loss: 0.7217 | Train F1: 0.4930 | Val Loss: 0.6518 | Val F1: 0.5566\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0138 | Train F1: 0.1936 | Val Loss: 0.9748 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9737 | Train F1: 0.1919 | Val Loss: 0.9263 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9478 | Train F1: 0.2064 | Val Loss: 0.8599 | Val F1: 0.2255\n",
      "Epoch 4/7 | Train Loss: 0.8777 | Train F1: 0.2850 | Val Loss: 0.7690 | Val F1: 0.3219\n",
      "Epoch 5/7 | Train Loss: 0.8158 | Train F1: 0.3394 | Val Loss: 0.7190 | Val F1: 0.3279\n",
      "Epoch 6/7 | Train Loss: 0.7643 | Train F1: 0.4262 | Val Loss: 0.6616 | Val F1: 0.5064\n",
      "Epoch 7/7 | Train Loss: 0.7272 | Train F1: 0.5003 | Val Loss: 0.6439 | Val F1: 0.5910\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0699 | Train F1: 0.2061 | Val Loss: 0.9958 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0260 | Train F1: 0.1930 | Val Loss: 0.9931 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0206 | Train F1: 0.1853 | Val Loss: 0.9975 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0180 | Train F1: 0.1856 | Val Loss: 0.9983 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0189 | Train F1: 0.1856 | Val Loss: 0.9920 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0113 | Train F1: 0.1855 | Val Loss: 0.9919 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0119 | Train F1: 0.1852 | Val Loss: 0.9817 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9948 | Train F1: 0.1909 | Val Loss: 0.9587 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9616 | Train F1: 0.2021 | Val Loss: 0.9129 | Val F1: 0.1917\n",
      "Epoch 3/7 | Train Loss: 0.9131 | Train F1: 0.2409 | Val Loss: 0.8392 | Val F1: 0.2464\n",
      "Epoch 4/7 | Train Loss: 0.8497 | Train F1: 0.3155 | Val Loss: 0.7755 | Val F1: 0.3214\n",
      "Epoch 5/7 | Train Loss: 0.8222 | Train F1: 0.3518 | Val Loss: 0.7547 | Val F1: 0.3447\n",
      "Epoch 6/7 | Train Loss: 0.7890 | Train F1: 0.3878 | Val Loss: 0.7318 | Val F1: 0.4145\n",
      "Epoch 7/7 | Train Loss: 0.7674 | Train F1: 0.4662 | Val Loss: 0.7022 | Val F1: 0.4606\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0330 | Train F1: 0.2034 | Val Loss: 0.9645 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9881 | Train F1: 0.1930 | Val Loss: 0.9530 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9750 | Train F1: 0.1971 | Val Loss: 0.9493 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9561 | Train F1: 0.2160 | Val Loss: 0.9199 | Val F1: 0.1948\n",
      "Epoch 5/7 | Train Loss: 0.9282 | Train F1: 0.2468 | Val Loss: 0.8654 | Val F1: 0.2427\n",
      "Epoch 6/7 | Train Loss: 0.9081 | Train F1: 0.2665 | Val Loss: 0.8579 | Val F1: 0.2616\n",
      "Epoch 7/7 | Train Loss: 0.8805 | Train F1: 0.3071 | Val Loss: 0.7966 | Val F1: 0.3376\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.1183 | Train F1: 0.2244 | Val Loss: 0.9767 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0202 | Train F1: 0.1926 | Val Loss: 0.9850 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0128 | Train F1: 0.1877 | Val Loss: 0.9777 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0031 | Train F1: 0.1861 | Val Loss: 0.9672 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0032 | Train F1: 0.1858 | Val Loss: 0.9652 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0040 | Train F1: 0.1909 | Val Loss: 0.9667 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 0.9983 | Train F1: 0.1870 | Val Loss: 0.9566 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0393 | Train F1: 0.1980 | Val Loss: 1.0121 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0321 | Train F1: 0.1948 | Val Loss: 1.0085 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0316 | Train F1: 0.1944 | Val Loss: 1.0315 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0384 | Train F1: 0.2010 | Val Loss: 1.0027 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0310 | Train F1: 0.1924 | Val Loss: 1.0092 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0263 | Train F1: 0.1900 | Val Loss: 1.0269 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0331 | Train F1: 0.1928 | Val Loss: 1.0397 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0372 | Train F1: 0.1923 | Val Loss: 1.0267 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0364 | Train F1: 0.1932 | Val Loss: 1.0078 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0362 | Train F1: 0.1941 | Val Loss: 1.0150 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0375 | Train F1: 0.2031 | Val Loss: 1.0031 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0480 | Train F1: 0.2040 | Val Loss: 1.0455 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0306 | Train F1: 0.1875 | Val Loss: 1.0436 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0366 | Train F1: 0.2034 | Val Loss: 1.0590 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0645 | Train F1: 0.2152 | Val Loss: 1.0048 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0570 | Train F1: 0.2160 | Val Loss: 0.9992 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0600 | Train F1: 0.2126 | Val Loss: 1.0479 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0508 | Train F1: 0.2043 | Val Loss: 1.0051 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0504 | Train F1: 0.1986 | Val Loss: 1.0063 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0420 | Train F1: 0.2075 | Val Loss: 1.0098 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0496 | Train F1: 0.2039 | Val Loss: 1.0114 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9735 | Train F1: 0.2108 | Val Loss: 0.8985 | Val F1: 0.2275\n",
      "Epoch 2/7 | Train Loss: 0.8612 | Train F1: 0.2993 | Val Loss: 0.7549 | Val F1: 0.3556\n",
      "Epoch 3/7 | Train Loss: 0.7692 | Train F1: 0.4427 | Val Loss: 0.6890 | Val F1: 0.5502\n",
      "Epoch 4/7 | Train Loss: 0.6900 | Train F1: 0.5812 | Val Loss: 0.6430 | Val F1: 0.5484\n",
      "Epoch 5/7 | Train Loss: 0.6457 | Train F1: 0.6176 | Val Loss: 0.6157 | Val F1: 0.5609\n",
      "Epoch 6/7 | Train Loss: 0.6167 | Train F1: 0.6460 | Val Loss: 0.5611 | Val F1: 0.6752\n",
      "Epoch 7/7 | Train Loss: 0.5851 | Train F1: 0.6672 | Val Loss: 0.5687 | Val F1: 0.6609\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 0.9990 | Train F1: 0.1971 | Val Loss: 0.9510 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9626 | Train F1: 0.2139 | Val Loss: 0.9377 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9546 | Train F1: 0.2304 | Val Loss: 0.9056 | Val F1: 0.2035\n",
      "Epoch 4/7 | Train Loss: 0.8943 | Train F1: 0.2767 | Val Loss: 0.7984 | Val F1: 0.2792\n",
      "Epoch 5/7 | Train Loss: 0.8373 | Train F1: 0.3115 | Val Loss: 0.7356 | Val F1: 0.3366\n",
      "Epoch 6/7 | Train Loss: 0.8007 | Train F1: 0.3827 | Val Loss: 0.7188 | Val F1: 0.3608\n",
      "Epoch 7/7 | Train Loss: 0.7525 | Train F1: 0.4335 | Val Loss: 0.6735 | Val F1: 0.4612\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0393 | Train F1: 0.2040 | Val Loss: 0.9880 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0156 | Train F1: 0.1899 | Val Loss: 0.9737 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0116 | Train F1: 0.1869 | Val Loss: 0.9841 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0074 | Train F1: 0.1852 | Val Loss: 0.9822 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0029 | Train F1: 0.1853 | Val Loss: 0.9676 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 0.9954 | Train F1: 0.1866 | Val Loss: 0.9751 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0024 | Train F1: 0.1903 | Val Loss: 0.9766 | Val F1: 0.1856\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9816 | Train F1: 0.2028 | Val Loss: 0.9131 | Val F1: 0.2031\n",
      "Epoch 2/7 | Train Loss: 0.9116 | Train F1: 0.2569 | Val Loss: 0.8357 | Val F1: 0.3150\n",
      "Epoch 3/7 | Train Loss: 0.8434 | Train F1: 0.3355 | Val Loss: 0.7310 | Val F1: 0.3720\n",
      "Epoch 4/7 | Train Loss: 0.7575 | Train F1: 0.4533 | Val Loss: 0.6785 | Val F1: 0.5061\n",
      "Epoch 5/7 | Train Loss: 0.6998 | Train F1: 0.5465 | Val Loss: 0.6267 | Val F1: 0.6136\n",
      "Epoch 6/7 | Train Loss: 0.6622 | Train F1: 0.5984 | Val Loss: 0.6123 | Val F1: 0.6070\n",
      "Epoch 7/7 | Train Loss: 0.6180 | Train F1: 0.6426 | Val Loss: 0.5819 | Val F1: 0.6389\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0094 | Train F1: 0.1915 | Val Loss: 0.9621 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9795 | Train F1: 0.2032 | Val Loss: 0.9411 | Val F1: 0.1998\n",
      "Epoch 3/7 | Train Loss: 0.9599 | Train F1: 0.2191 | Val Loss: 0.9286 | Val F1: 0.2081\n",
      "Epoch 4/7 | Train Loss: 0.9445 | Train F1: 0.2305 | Val Loss: 0.8921 | Val F1: 0.2902\n",
      "Epoch 5/7 | Train Loss: 0.9108 | Train F1: 0.2561 | Val Loss: 0.8493 | Val F1: 0.2578\n",
      "Epoch 6/7 | Train Loss: 0.8729 | Train F1: 0.3035 | Val Loss: 0.7940 | Val F1: 0.3060\n",
      "Epoch 7/7 | Train Loss: 0.8239 | Train F1: 0.3597 | Val Loss: 0.7469 | Val F1: 0.3895\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0659 | Train F1: 0.2100 | Val Loss: 0.9972 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0200 | Train F1: 0.1981 | Val Loss: 0.9833 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0120 | Train F1: 0.1888 | Val Loss: 0.9748 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0054 | Train F1: 0.1861 | Val Loss: 0.9666 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 0.9935 | Train F1: 0.1856 | Val Loss: 0.9675 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 0.9922 | Train F1: 0.1871 | Val Loss: 0.9600 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 0.9883 | Train F1: 0.1918 | Val Loss: 0.9502 | Val F1: 0.1856\n",
      "\n",
      "üèÜ Top 3 configurations:\n",
      "\n",
      "#1 - Val F1: 0.6752\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 128\n",
      "  learning_rate: 0.001\n",
      "  dropout: 0.1\n",
      "Train F1: 0.6460 | Val Loss: 0.5611\n",
      "\n",
      "#2 - Val F1: 0.6389\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 128\n",
      "  learning_rate: 0.0005\n",
      "  dropout: 0.1\n",
      "Train F1: 0.6426 | Val Loss: 0.5819\n",
      "\n",
      "#3 - Val F1: 0.5910\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 64\n",
      "  learning_rate: 0.001\n",
      "  dropout: 0.2\n",
      "Train F1: 0.5003 | Val Loss: 0.6439\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_grid = {\n",
    "    \"lst_hidden_size\": [32, 64, 128],\n",
    "    \n",
    "    \"learning_rate\": [.01, 0.001, 0.0005],\n",
    "    \n",
    "    \"dropout\": [0.1, 0.2, .5],\n",
    "}\n",
    "fixed = {\n",
    "    \"num_classes\": 4,\n",
    "    \"signal_length\": X_train.shape[1],\n",
    "    \"n_fft\": 512,\n",
    "    \"hop_length\": 256,\n",
    "    \"conv1_padding\": 1,\n",
    "    \"conv2_padding\": 1,\n",
    "    \"conv1_kernel\": 3,\n",
    "    \"conv2_kernel\": 3,\n",
    "    \"lstm_num_layers\": 1,\n",
    "    \"conv1_channels\": 32,\n",
    "    \"conv2_channels\": 32\n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    ECGNet,\n",
    "    param_grid,\n",
    "    fixed,\n",
    "    device=device,\n",
    "    epochs=7,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16745e33",
   "metadata": {},
   "source": [
    "### üß™ Best Hyperparameter Configuration for CNN-LSTM model\n",
    "\n",
    "After performing an extensive grid search over several key hyperparameters of the `CNN-LSTM` architecture, the best-performing configuration (based on validation F1-score) was identified as:\n",
    "\n",
    "- `dropout = .1`\n",
    "- `lst_hidden_size = 128`\n",
    "- `learning_rate = 0.001`\n",
    "\n",
    "This combination yielded the highest balance between training and validation performance, indicating that the model can generalize well without overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Final Training Setup\n",
    "\n",
    "With the optimal hyperparameters selected, we now proceed to **train the final version of CNN-LSTM** using a larger number of epochs (50) to fully exploit the model's capacity:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "219adc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9872 - Train F1: 0.1904 - Val Loss: 0.9549 - Val F1: 0.1856\n",
      "Epoch 2/50 - Train Loss: 0.9387 - Train F1: 0.2269 - Val Loss: 0.9151 - Val F1: 0.3043\n",
      "Epoch 3/50 - Train Loss: 0.8768 - Train F1: 0.2833 - Val Loss: 0.8154 - Val F1: 0.3088\n",
      "Epoch 4/50 - Train Loss: 0.7838 - Train F1: 0.4139 - Val Loss: 0.7662 - Val F1: 0.3663\n",
      "Epoch 5/50 - Train Loss: 0.7129 - Train F1: 0.5202 - Val Loss: 0.6318 - Val F1: 0.5549\n",
      "Epoch 6/50 - Train Loss: 0.6683 - Train F1: 0.5611 - Val Loss: 0.6131 - Val F1: 0.5908\n",
      "Epoch 7/50 - Train Loss: 0.6334 - Train F1: 0.6164 - Val Loss: 0.5845 - Val F1: 0.6763\n",
      "Epoch 8/50 - Train Loss: 0.5986 - Train F1: 0.6598 - Val Loss: 0.5708 - Val F1: 0.6665\n",
      "Epoch 9/50 - Train Loss: 0.5900 - Train F1: 0.6433 - Val Loss: 0.5662 - Val F1: 0.6876\n",
      "Epoch 10/50 - Train Loss: 0.5613 - Train F1: 0.6833 - Val Loss: 0.5501 - Val F1: 0.7191\n",
      "Epoch 11/50 - Train Loss: 0.5487 - Train F1: 0.7017 - Val Loss: 0.5361 - Val F1: 0.7054\n",
      "Epoch 12/50 - Train Loss: 0.5321 - Train F1: 0.7083 - Val Loss: 0.5369 - Val F1: 0.6608\n",
      "Epoch 13/50 - Train Loss: 0.5114 - Train F1: 0.7269 - Val Loss: 0.5368 - Val F1: 0.7068\n",
      "Epoch 14/50 - Train Loss: 0.5031 - Train F1: 0.7340 - Val Loss: 0.5312 - Val F1: 0.7233\n",
      "Epoch 15/50 - Train Loss: 0.4883 - Train F1: 0.7388 - Val Loss: 0.5252 - Val F1: 0.7219\n",
      "Epoch 16/50 - Train Loss: 0.4695 - Train F1: 0.7573 - Val Loss: 0.4939 - Val F1: 0.7370\n",
      "Epoch 17/50 - Train Loss: 0.4616 - Train F1: 0.7671 - Val Loss: 0.4930 - Val F1: 0.7265\n",
      "Epoch 18/50 - Train Loss: 0.4425 - Train F1: 0.7698 - Val Loss: 0.5043 - Val F1: 0.7279\n",
      "Epoch 19/50 - Train Loss: 0.4346 - Train F1: 0.7758 - Val Loss: 0.5030 - Val F1: 0.7627\n",
      "Epoch 20/50 - Train Loss: 0.4263 - Train F1: 0.7818 - Val Loss: 0.5344 - Val F1: 0.7298\n",
      "Epoch 21/50 - Train Loss: 0.4082 - Train F1: 0.7991 - Val Loss: 0.5018 - Val F1: 0.7558\n",
      "\n",
      "Early stopping triggered at epoch 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.8777    0.8898    0.8837       363\n",
      "     class_1     0.6897    0.7407    0.7143        54\n",
      "     class_2     0.7091    0.6648    0.6862       176\n",
      "     class_3     0.7083    0.7727    0.7391        22\n",
      "\n",
      "    accuracy                         0.8081       615\n",
      "   macro avg     0.7462    0.7670    0.7558       615\n",
      "weighted avg     0.8069    0.8081    0.8071       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECGNet(\n",
    "    num_classes=4,\n",
    "    n_fft=512,\n",
    "    hop_length=256,\n",
    "    conv1_padding=1,\n",
    "    conv2_padding=1,\n",
    "    conv1_kernel=3,\n",
    "    conv2_kernel=3,\n",
    "    lstm_num_layers=1,\n",
    "    conv1_channels=32,\n",
    "    conv2_channels=32,\n",
    "    lst_hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    signal_length=X_train.shape[1],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, augment_data=False, device=device)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "# Guardar como base.csv (puedes cambiar el nombre a 'augment.csv' o 'reduced.csv')\n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "\n",
    "df.to_csv('base.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585c602",
   "metadata": {},
   "source": [
    "### üîç Hyperparameter Search and Model Selection (TCN)\n",
    "\n",
    "In this section, we perform **hyperparameter tuning** for the `TCN_STFT_Classifier` model, which combines a Temporal Convolutional Network (TCN). The goal is to identify the best-performing configuration of this architecture on the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚öôÔ∏è How it Works\n",
    "\n",
    "1. We define a **grid of hyperparameters** to test, including:\n",
    "   - `dropout` rates\n",
    "   - TCN `hidden_channels` per layer\n",
    "   - `kernel size`\n",
    "   - `num levels`\n",
    "2. Using `itertools.product`, we generate all possible **combinations** from this grid.\n",
    "3. For each combination:\n",
    "   - Instantiate the model with the current hyperparameters.\n",
    "   - Train it for a fixed number of epochs (e.g., 10) on the training set.\n",
    "   - Evaluate its performance using **F1-score** and **loss** on the validation set.\n",
    "4. Track the best configuration based on **validation F1-score**.\n",
    "\n",
    "---\n",
    "\n",
    "This tuning process ensures that our TCN model, augmented with time-frequency features from STFT, is both **optimized** and **robust** to overfitting or underfitting. It complements the tuning process performed on our first model (`ECGNet`), allowing us to compare both architectures fairly under their best conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f232fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\AMLS\\venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.8957 | Train F1: 0.3041 | Val Loss: 0.7980 | Val F1: 0.4038\n",
      "Epoch 2/5 | Train Loss: 0.7564 | Train F1: 0.4473 | Val Loss: 0.7375 | Val F1: 0.4446\n",
      "Epoch 3/5 | Train Loss: 0.6787 | Train F1: 0.5305 | Val Loss: 0.6653 | Val F1: 0.6193\n",
      "Epoch 4/5 | Train Loss: 0.6359 | Train F1: 0.5815 | Val Loss: 0.6691 | Val F1: 0.5076\n",
      "Epoch 5/5 | Train Loss: 0.6122 | Train F1: 0.5919 | Val Loss: 0.6083 | Val F1: 0.5734\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\AMLS\\venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.9070 | Train F1: 0.2869 | Val Loss: 0.8204 | Val F1: 0.3681\n",
      "Epoch 2/5 | Train Loss: 0.7834 | Train F1: 0.4224 | Val Loss: 0.7378 | Val F1: 0.4814\n",
      "Epoch 3/5 | Train Loss: 0.7238 | Train F1: 0.4984 | Val Loss: 0.6812 | Val F1: 0.5096\n",
      "Epoch 4/5 | Train Loss: 0.6698 | Train F1: 0.5475 | Val Loss: 0.6347 | Val F1: 0.4769\n",
      "Epoch 5/5 | Train Loss: 0.6413 | Train F1: 0.5732 | Val Loss: 0.6350 | Val F1: 0.5744\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9172 | Train F1: 0.2912 | Val Loss: 0.8757 | Val F1: 0.3684\n",
      "Epoch 2/5 | Train Loss: 0.8034 | Train F1: 0.4009 | Val Loss: 0.7960 | Val F1: 0.4224\n",
      "Epoch 3/5 | Train Loss: 0.7169 | Train F1: 0.4890 | Val Loss: 0.6728 | Val F1: 0.5141\n",
      "Epoch 4/5 | Train Loss: 0.6523 | Train F1: 0.5715 | Val Loss: 0.6141 | Val F1: 0.6033\n",
      "Epoch 5/5 | Train Loss: 0.6229 | Train F1: 0.5953 | Val Loss: 0.6177 | Val F1: 0.5955\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9312 | Train F1: 0.2619 | Val Loss: 0.8537 | Val F1: 0.3371\n",
      "Epoch 2/5 | Train Loss: 0.8025 | Train F1: 0.4194 | Val Loss: 0.7514 | Val F1: 0.4283\n",
      "Epoch 3/5 | Train Loss: 0.7157 | Train F1: 0.5299 | Val Loss: 0.7347 | Val F1: 0.4631\n",
      "Epoch 4/5 | Train Loss: 0.6628 | Train F1: 0.5616 | Val Loss: 0.6189 | Val F1: 0.5939\n",
      "Epoch 5/5 | Train Loss: 0.6254 | Train F1: 0.5934 | Val Loss: 0.6146 | Val F1: 0.5390\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.8967 | Train F1: 0.3040 | Val Loss: 0.8040 | Val F1: 0.4118\n",
      "Epoch 2/5 | Train Loss: 0.7595 | Train F1: 0.4320 | Val Loss: 0.6950 | Val F1: 0.4264\n",
      "Epoch 3/5 | Train Loss: 0.6987 | Train F1: 0.5022 | Val Loss: 0.6777 | Val F1: 0.5391\n",
      "Epoch 4/5 | Train Loss: 0.6543 | Train F1: 0.5551 | Val Loss: 0.6217 | Val F1: 0.5853\n",
      "Epoch 5/5 | Train Loss: 0.6350 | Train F1: 0.5983 | Val Loss: 0.6618 | Val F1: 0.5545\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9178 | Train F1: 0.2541 | Val Loss: 0.8680 | Val F1: 0.3173\n",
      "Epoch 2/5 | Train Loss: 0.7952 | Train F1: 0.4262 | Val Loss: 0.8095 | Val F1: 0.4508\n",
      "Epoch 3/5 | Train Loss: 0.7308 | Train F1: 0.4912 | Val Loss: 0.6845 | Val F1: 0.5073\n",
      "Epoch 4/5 | Train Loss: 0.6704 | Train F1: 0.5592 | Val Loss: 0.6998 | Val F1: 0.4729\n",
      "Epoch 5/5 | Train Loss: 0.6389 | Train F1: 0.5662 | Val Loss: 0.6300 | Val F1: 0.5820\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9526 | Train F1: 0.2661 | Val Loss: 0.8629 | Val F1: 0.3155\n",
      "Epoch 2/5 | Train Loss: 0.8133 | Train F1: 0.3860 | Val Loss: 0.7584 | Val F1: 0.4543\n",
      "Epoch 3/5 | Train Loss: 0.7664 | Train F1: 0.4611 | Val Loss: 0.7258 | Val F1: 0.4504\n",
      "Epoch 4/5 | Train Loss: 0.6995 | Train F1: 0.5153 | Val Loss: 0.6663 | Val F1: 0.5052\n",
      "Epoch 5/5 | Train Loss: 0.6571 | Train F1: 0.5624 | Val Loss: 0.6697 | Val F1: 0.4931\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9232 | Train F1: 0.2853 | Val Loss: 0.8424 | Val F1: 0.4135\n",
      "Epoch 2/5 | Train Loss: 0.7727 | Train F1: 0.4630 | Val Loss: 0.7353 | Val F1: 0.5212\n",
      "Epoch 3/5 | Train Loss: 0.7223 | Train F1: 0.5295 | Val Loss: 0.6872 | Val F1: 0.4692\n",
      "Epoch 4/5 | Train Loss: 0.6731 | Train F1: 0.5499 | Val Loss: 0.6144 | Val F1: 0.6248\n",
      "Epoch 5/5 | Train Loss: 0.6297 | Train F1: 0.5890 | Val Loss: 0.5849 | Val F1: 0.6317\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9078 | Train F1: 0.2937 | Val Loss: 0.9274 | Val F1: 0.2939\n",
      "Epoch 2/5 | Train Loss: 0.8227 | Train F1: 0.4092 | Val Loss: 0.8512 | Val F1: 0.3397\n",
      "Epoch 3/5 | Train Loss: 0.7520 | Train F1: 0.4835 | Val Loss: 0.7375 | Val F1: 0.4429\n",
      "Epoch 4/5 | Train Loss: 0.7184 | Train F1: 0.5124 | Val Loss: 0.6843 | Val F1: 0.4525\n",
      "Epoch 5/5 | Train Loss: 0.6875 | Train F1: 0.5391 | Val Loss: 0.6628 | Val F1: 0.5620\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9110 | Train F1: 0.2873 | Val Loss: 0.8330 | Val F1: 0.3698\n",
      "Epoch 2/5 | Train Loss: 0.7845 | Train F1: 0.4075 | Val Loss: 0.7812 | Val F1: 0.3652\n",
      "Epoch 3/5 | Train Loss: 0.7108 | Train F1: 0.4814 | Val Loss: 0.6800 | Val F1: 0.4883\n",
      "Epoch 4/5 | Train Loss: 0.6868 | Train F1: 0.5220 | Val Loss: 0.6521 | Val F1: 0.4392\n",
      "Epoch 5/5 | Train Loss: 0.6491 | Train F1: 0.5668 | Val Loss: 0.6985 | Val F1: 0.5298\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9387 | Train F1: 0.2584 | Val Loss: 0.8604 | Val F1: 0.3929\n",
      "Epoch 2/5 | Train Loss: 0.8250 | Train F1: 0.3931 | Val Loss: 0.7924 | Val F1: 0.4902\n",
      "Epoch 3/5 | Train Loss: 0.7698 | Train F1: 0.4746 | Val Loss: 0.7898 | Val F1: 0.5400\n",
      "Epoch 4/5 | Train Loss: 0.7008 | Train F1: 0.5172 | Val Loss: 0.6671 | Val F1: 0.4774\n",
      "Epoch 5/5 | Train Loss: 0.6660 | Train F1: 0.5704 | Val Loss: 0.6403 | Val F1: 0.4913\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9422 | Train F1: 0.2585 | Val Loss: 0.8647 | Val F1: 0.3084\n",
      "Epoch 2/5 | Train Loss: 0.8200 | Train F1: 0.3993 | Val Loss: 0.8141 | Val F1: 0.4274\n",
      "Epoch 3/5 | Train Loss: 0.7506 | Train F1: 0.4777 | Val Loss: 0.7028 | Val F1: 0.4299\n",
      "Epoch 4/5 | Train Loss: 0.6990 | Train F1: 0.5228 | Val Loss: 0.6695 | Val F1: 0.5761\n",
      "Epoch 5/5 | Train Loss: 0.6752 | Train F1: 0.5483 | Val Loss: 0.7029 | Val F1: 0.5054\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9178 | Train F1: 0.3062 | Val Loss: 0.7936 | Val F1: 0.3913\n",
      "Epoch 2/5 | Train Loss: 0.7644 | Train F1: 0.4421 | Val Loss: 0.7232 | Val F1: 0.4500\n",
      "Epoch 3/5 | Train Loss: 0.6881 | Train F1: 0.5132 | Val Loss: 0.7278 | Val F1: 0.4936\n",
      "Epoch 4/5 | Train Loss: 0.6487 | Train F1: 0.5537 | Val Loss: 0.6602 | Val F1: 0.5552\n",
      "Epoch 5/5 | Train Loss: 0.6151 | Train F1: 0.5984 | Val Loss: 0.6149 | Val F1: 0.5873\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9322 | Train F1: 0.3194 | Val Loss: 0.9377 | Val F1: 0.2792\n",
      "Epoch 2/5 | Train Loss: 0.8029 | Train F1: 0.4462 | Val Loss: 0.7147 | Val F1: 0.5170\n",
      "Epoch 3/5 | Train Loss: 0.7109 | Train F1: 0.4914 | Val Loss: 0.6911 | Val F1: 0.5875\n",
      "Epoch 4/5 | Train Loss: 0.6561 | Train F1: 0.5407 | Val Loss: 0.6399 | Val F1: 0.5402\n",
      "Epoch 5/5 | Train Loss: 0.6293 | Train F1: 0.5752 | Val Loss: 0.6340 | Val F1: 0.5842\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9388 | Train F1: 0.3014 | Val Loss: 0.8250 | Val F1: 0.3590\n",
      "Epoch 2/5 | Train Loss: 0.7941 | Train F1: 0.4375 | Val Loss: 0.7554 | Val F1: 0.4329\n",
      "Epoch 3/5 | Train Loss: 0.7277 | Train F1: 0.5123 | Val Loss: 0.7125 | Val F1: 0.5344\n",
      "Epoch 4/5 | Train Loss: 0.6736 | Train F1: 0.5372 | Val Loss: 0.6553 | Val F1: 0.5290\n",
      "Epoch 5/5 | Train Loss: 0.6178 | Train F1: 0.6061 | Val Loss: 0.6421 | Val F1: 0.5283\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 1.0012 | Train F1: 0.2685 | Val Loss: 0.8890 | Val F1: 0.2696\n",
      "Epoch 2/5 | Train Loss: 0.8215 | Train F1: 0.4033 | Val Loss: 0.7766 | Val F1: 0.4729\n",
      "Epoch 3/5 | Train Loss: 0.7611 | Train F1: 0.4493 | Val Loss: 0.7576 | Val F1: 0.4654\n",
      "Epoch 4/5 | Train Loss: 0.6968 | Train F1: 0.5255 | Val Loss: 0.6717 | Val F1: 0.5882\n",
      "Epoch 5/5 | Train Loss: 0.6557 | Train F1: 0.5821 | Val Loss: 0.6524 | Val F1: 0.5759\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9576 | Train F1: 0.2944 | Val Loss: 0.8636 | Val F1: 0.4432\n",
      "Epoch 2/5 | Train Loss: 0.8099 | Train F1: 0.4308 | Val Loss: 0.7688 | Val F1: 0.4124\n",
      "Epoch 3/5 | Train Loss: 0.7408 | Train F1: 0.4911 | Val Loss: 0.7036 | Val F1: 0.4547\n",
      "Epoch 4/5 | Train Loss: 0.6964 | Train F1: 0.5420 | Val Loss: 0.6917 | Val F1: 0.4852\n",
      "Epoch 5/5 | Train Loss: 0.6513 | Train F1: 0.5543 | Val Loss: 0.6419 | Val F1: 0.6107\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9160 | Train F1: 0.3130 | Val Loss: 0.8014 | Val F1: 0.3877\n",
      "Epoch 2/5 | Train Loss: 0.7768 | Train F1: 0.4585 | Val Loss: 0.7542 | Val F1: 0.4746\n",
      "Epoch 3/5 | Train Loss: 0.7098 | Train F1: 0.5104 | Val Loss: 0.7146 | Val F1: 0.4827\n",
      "Epoch 4/5 | Train Loss: 0.6610 | Train F1: 0.5584 | Val Loss: 0.6396 | Val F1: 0.6068\n",
      "Epoch 5/5 | Train Loss: 0.6437 | Train F1: 0.5841 | Val Loss: 0.6175 | Val F1: 0.5325\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9263 | Train F1: 0.3000 | Val Loss: 0.8216 | Val F1: 0.4293\n",
      "Epoch 2/5 | Train Loss: 0.7876 | Train F1: 0.4540 | Val Loss: 0.7569 | Val F1: 0.4162\n",
      "Epoch 3/5 | Train Loss: 0.7180 | Train F1: 0.4909 | Val Loss: 0.6672 | Val F1: 0.5912\n",
      "Epoch 4/5 | Train Loss: 0.6550 | Train F1: 0.5760 | Val Loss: 0.6433 | Val F1: 0.5795\n",
      "Epoch 5/5 | Train Loss: 0.6242 | Train F1: 0.5950 | Val Loss: 0.6487 | Val F1: 0.5243\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 1.0277 | Train F1: 0.2476 | Val Loss: 0.8726 | Val F1: 0.3160\n",
      "Epoch 2/5 | Train Loss: 0.8246 | Train F1: 0.3968 | Val Loss: 0.7716 | Val F1: 0.4529\n",
      "Epoch 3/5 | Train Loss: 0.7660 | Train F1: 0.4768 | Val Loss: 0.8121 | Val F1: 0.4609\n",
      "Epoch 4/5 | Train Loss: 0.7205 | Train F1: 0.5295 | Val Loss: 0.7027 | Val F1: 0.5931\n",
      "Epoch 5/5 | Train Loss: 0.6946 | Train F1: 0.5591 | Val Loss: 0.6829 | Val F1: 0.4902\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9471 | Train F1: 0.2966 | Val Loss: 0.8442 | Val F1: 0.3768\n",
      "Epoch 2/5 | Train Loss: 0.8039 | Train F1: 0.4393 | Val Loss: 0.7625 | Val F1: 0.4620\n",
      "Epoch 3/5 | Train Loss: 0.7531 | Train F1: 0.4875 | Val Loss: 0.7500 | Val F1: 0.4908\n",
      "Epoch 4/5 | Train Loss: 0.6816 | Train F1: 0.5296 | Val Loss: 0.6770 | Val F1: 0.5475\n",
      "Epoch 5/5 | Train Loss: 0.6751 | Train F1: 0.5479 | Val Loss: 0.7081 | Val F1: 0.4993\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9606 | Train F1: 0.2970 | Val Loss: 0.8969 | Val F1: 0.2966\n",
      "Epoch 2/5 | Train Loss: 0.8345 | Train F1: 0.3672 | Val Loss: 0.7790 | Val F1: 0.4139\n",
      "Epoch 3/5 | Train Loss: 0.7586 | Train F1: 0.4757 | Val Loss: 0.7167 | Val F1: 0.5484\n",
      "Epoch 4/5 | Train Loss: 0.7025 | Train F1: 0.5353 | Val Loss: 0.6878 | Val F1: 0.5536\n",
      "Epoch 5/5 | Train Loss: 0.6625 | Train F1: 0.5614 | Val Loss: 0.6267 | Val F1: 0.5827\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 0.9467 | Train F1: 0.2860 | Val Loss: 0.8605 | Val F1: 0.3371\n",
      "Epoch 2/5 | Train Loss: 0.8269 | Train F1: 0.4189 | Val Loss: 0.7650 | Val F1: 0.4295\n",
      "Epoch 3/5 | Train Loss: 0.7427 | Train F1: 0.4883 | Val Loss: 0.7165 | Val F1: 0.5573\n",
      "Epoch 4/5 | Train Loss: 0.6784 | Train F1: 0.5418 | Val Loss: 0.6544 | Val F1: 0.5813\n",
      "Epoch 5/5 | Train Loss: 0.6667 | Train F1: 0.5455 | Val Loss: 0.6449 | Val F1: 0.5685\n",
      "\n",
      "üîß Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Learning rate: 0.001\n",
      "Epoch 1/5 | Train Loss: 1.0059 | Train F1: 0.2624 | Val Loss: 0.8881 | Val F1: 0.3419\n",
      "Epoch 2/5 | Train Loss: 0.8281 | Train F1: 0.3978 | Val Loss: 0.8408 | Val F1: 0.3992\n",
      "Epoch 3/5 | Train Loss: 0.7816 | Train F1: 0.4630 | Val Loss: 0.7435 | Val F1: 0.4647\n",
      "Epoch 4/5 | Train Loss: 0.7231 | Train F1: 0.5209 | Val Loss: 0.7178 | Val F1: 0.4822\n",
      "Epoch 5/5 | Train Loss: 0.6937 | Train F1: 0.5381 | Val Loss: 0.6442 | Val F1: 0.5113\n",
      "\n",
      "üèÜ Top 3 configurations:\n",
      "\n",
      "#1 - Val F1: 0.6317 (reached at epoch 5)\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 5\n",
      "  hidden_channels: [64, 128, 128, 128]\n",
      "  dropout: 0.2\n",
      "  num_levels: 4\n",
      "  learning_rate: 0.001\n",
      "Train F1: 0.5890 | Val Loss: 0.5849\n",
      "\n",
      "#2 - Val F1: 0.6193 (reached at epoch 3)\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 3\n",
      "  hidden_channels: [64, 128, 128]\n",
      "  dropout: 0.1\n",
      "  num_levels: 3\n",
      "  learning_rate: 0.001\n",
      "Train F1: 0.5305 | Val Loss: 0.6653\n",
      "\n",
      "#3 - Val F1: 0.6107 (reached at epoch 5)\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 3\n",
      "  hidden_channels: [128, 128, 128]\n",
      "  dropout: 0.2\n",
      "  num_levels: 3\n",
      "  learning_rate: 0.001\n",
      "Train F1: 0.5543 | Val Loss: 0.6419\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # Configuraciones donde len(hidden_channels) == num_levels\n",
    "    \n",
    "        'hidden_channels': [[64,128,128,128],[128,128,128,128]],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'kernel_size': [3, 5],\n",
    "    'num_levels': [3,4]\n",
    "}\n",
    "\n",
    "\n",
    "fixed = {\n",
    "    \"num_classes\": 4,\n",
    "    \"n_fft\": 256,\n",
    "    \"hop_length\": 128,\n",
    "    \"kernel_size\": 3,\n",
    "    \"learning_rate\" : .001,\n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    TCN_STFT_Classifier,\n",
    "    param_grid,\n",
    "    fixed,\n",
    "    device=device,\n",
    "    epochs=5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208c4b43",
   "metadata": {},
   "source": [
    "### üèÅ Final Model Selection: (TCN)\n",
    "\n",
    "After conducting an extensive grid search over multiple combinations of hyperparameters, we identified the **best-performing configuration** for the `(TCN)` model.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß Best Hyperparameters Found\n",
    "\n",
    "- `kernel size = 3`\n",
    "- `num levels = 3`\n",
    "- `dropout = 128`\n",
    "- `hidden channels = [64, 128, 128]`\n",
    "\n",
    "These hyperparameters achieved the **highest F1-score** on the validation set among all tested configurations.\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ Final Training Phase\n",
    "\n",
    "Using this optimal setup, we now train the final version of the `TCN` model using **50 epochs** to allow the model to fully converge and leverage the learned configuration. This final model is expected to yield improved generalization performance and serve as a strong baseline for comparison against the first architecture (`CNN_LSTM`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f03486",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\AMLS\\venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9455 - Train F1: 0.2654 - Val Loss: 0.7854 - Val F1: 0.3621\n",
      "Epoch 2/50 - Train Loss: 0.7761 - Train F1: 0.4506 - Val Loss: 0.7217 - Val F1: 0.5176\n",
      "Epoch 3/50 - Train Loss: 0.7156 - Train F1: 0.4853 - Val Loss: 0.6634 - Val F1: 0.5686\n",
      "Epoch 4/50 - Train Loss: 0.6795 - Train F1: 0.5269 - Val Loss: 0.6553 - Val F1: 0.5142\n",
      "Epoch 5/50 - Train Loss: 0.6405 - Train F1: 0.5714 - Val Loss: 0.6434 - Val F1: 0.4684\n",
      "Epoch 6/50 - Train Loss: 0.6368 - Train F1: 0.5711 - Val Loss: 0.6879 - Val F1: 0.5538\n",
      "Epoch 7/50 - Train Loss: 0.6140 - Train F1: 0.5873 - Val Loss: 0.6863 - Val F1: 0.4769\n",
      "Epoch 8/50 - Train Loss: 0.5865 - Train F1: 0.6157 - Val Loss: 0.6351 - Val F1: 0.4949\n",
      "Epoch 9/50 - Train Loss: 0.5819 - Train F1: 0.6471 - Val Loss: 0.5858 - Val F1: 0.6015\n",
      "Epoch 10/50 - Train Loss: 0.5536 - Train F1: 0.6607 - Val Loss: 0.5457 - Val F1: 0.6634\n",
      "Epoch 11/50 - Train Loss: 0.5548 - Train F1: 0.6483 - Val Loss: 0.5744 - Val F1: 0.5991\n",
      "Epoch 12/50 - Train Loss: 0.5391 - Train F1: 0.6626 - Val Loss: 0.5687 - Val F1: 0.6919\n",
      "Epoch 13/50 - Train Loss: 0.5326 - Train F1: 0.6827 - Val Loss: 0.5570 - Val F1: 0.6309\n",
      "Epoch 14/50 - Train Loss: 0.5125 - Train F1: 0.6959 - Val Loss: 0.5753 - Val F1: 0.6783\n",
      "Epoch 15/50 - Train Loss: 0.4959 - Train F1: 0.7092 - Val Loss: 0.5894 - Val F1: 0.6601\n",
      "\n",
      "Early stopping triggered at epoch 15\n",
      "0.7149182558059692 0.6601388454437256\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TCN_STFT_Classifier(\n",
    "    num_classes=4,\n",
    "    hop_length = 128,\n",
    "    n_fft = 256,\n",
    "    kernel_size = 3, \n",
    "    hidden_channels=  [64, 128, 128],\n",
    "    dropout = 0.2,\n",
    "    num_levels = 3,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, augment_data=False, device=device)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "        \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "\n",
    "\n",
    "# Guardar como base.csv (puedes cambiar el nombre a 'augment.csv' o 'reduced.csv')\n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "\n",
    "df.to_csv('base.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
