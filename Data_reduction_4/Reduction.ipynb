{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202f318d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Project at: c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch as torch\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # move up one level from notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "# Visual confirmation\n",
    "print(\"[✓] Project at:\", PROJECT_ROOT)\n",
    "\n",
    "from src.models.model_1 import ECGNet\n",
    "from src.models.hyperparamter_tunning import hyperparameter_search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e09000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root path if not already present\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from src.data.load_data import load_train_data, EDGCDataset, load_test_data, EDGCTestDataset\n",
    "from src.data.stratified_split import stratified_split_pad_torch, pad_test_torch\n",
    "from src.models.model_trainer import Trainer\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0777e94",
   "metadata": {},
   "source": [
    "### This part implements an import of the dataset but with a lossy technique. Afther this, we will reevaluate the same model we had with this new data. \n",
    "\n",
    "Most of the functions necessary for this compression are stored with the rest of the source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c3a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.lossless_compression import write_compressed_file, read_compressed_file\n",
    "from src.data.lossy_compression import read_custom_compressed, compress_and_save, CONFIG\n",
    "\n",
    "def lossy_import():\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train = load_train_data()\n",
    "    \n",
    "    use_lossy = True\n",
    "    \n",
    "    if use_lossy:\n",
    "        compress_and_save(X_train, CONFIG)\n",
    "        compressed_path = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"compressed_data.bin\")\n",
    "        X_train_custom = read_custom_compressed(compressed_path)\n",
    "        \n",
    "        X_tensor_list = [torch.tensor(x, dtype=torch.float32) for x in X_train_custom]\n",
    "        X_train_pad = pad_sequence(X_tensor_list, batch_first=True)\n",
    "        lengths_train_final = torch.tensor([len(x) for x in X_train_custom])\n",
    "        y_train_tensor = torch.tensor(y_train.iloc[:, 0].values, dtype=torch.long)\n",
    "\n",
    "    durations = np.array([len(x) / 300 for x in X_train])\n",
    "\n",
    "    print(\"Splitting and padding...\")\n",
    "\n",
    "    X_train, X_val, lengths_train, lengths_val, y_train, y_val = (\n",
    "        stratified_split_pad_torch(X_train, y_train)\n",
    "    )     \n",
    "    \n",
    "    train_dataset = EDGCDataset(X_train_pad, lengths_train_final, y_train_tensor) if use_lossy else EDGCDataset(X_train, lengths_train, y_train)\n",
    "    val_dataset = EDGCDataset(X_val, lengths_val, y_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    for X_batch, lengths_batch, y_batch in train_loader:\n",
    "        print(\n",
    "            f\"Batch shapes → X: {X_batch.shape}, lengths: {lengths_batch.shape}, y: {y_batch.shape}\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print(\"Data preparation completed.\")\n",
    "\n",
    "    return train_loader, val_loader, X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6626910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "[✓] Loaded X_train with 6179 sequences\n",
      "[✓] Loaded y_train with shape (6179, 1)\n",
      "Splitting and padding...\n",
      "Batch shapes → X: torch.Size([32, 18286]), lengths: torch.Size([32]), y: torch.Size([32])\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader,signal_size = lossy_import()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccfa1c",
   "metadata": {},
   "source": [
    "## Next steps are necessary for evaluating the model, and because the model requires to give the number of columns of the X_train matrix after padding as a paramaeter for the model. However the compressed data was already previously generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42b209c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Loaded X_test with 2649 sequences\n",
      "[✓] Loaded X_train with 6179 sequences\n",
      "[✓] Loaded y_train with shape (6179, 1)\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_1 import ECGNet\n",
    "from src.models.hyperparamter_tunning import hyperparameter_search\n",
    "X_test = load_test_data()\n",
    "\n",
    "X_test, lengths_test = pad_test_torch(X_test)\n",
    "\n",
    "test_dataset = EDGCTestDataset(X_test, lengths_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "X_train, y_train = load_train_data()\n",
    "\n",
    "X_train, X_val, lengths_train, lengths_val, y_train, y_val = stratified_split_pad_torch(\n",
    "    X_train, y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95374640",
   "metadata": {},
   "source": [
    "## Retraining and evaluating model with lossy compressed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "459218b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 1.0416 - Train F1: 0.2069 - Val Loss: 0.9790 - Val F1: 0.1856\n",
      "Epoch 2/50 - Train Loss: 0.9987 - Train F1: 0.1865 - Val Loss: 0.9890 - Val F1: 0.1856\n",
      "Epoch 3/50 - Train Loss: 1.0053 - Train F1: 0.1858 - Val Loss: 1.0004 - Val F1: 0.1856\n",
      "Epoch 4/50 - Train Loss: 1.0077 - Train F1: 0.1859 - Val Loss: 1.0009 - Val F1: 0.1856\n",
      "Epoch 5/50 - Train Loss: 1.0056 - Train F1: 0.1865 - Val Loss: 0.9960 - Val F1: 0.1856\n",
      "Epoch 6/50 - Train Loss: 1.0045 - Train F1: 0.1853 - Val Loss: 0.9988 - Val F1: 0.1856\n",
      "\n",
      "Early stopping triggered at epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.5902    1.0000    0.7423       363\n",
      "     class_1     0.0000    0.0000    0.0000        54\n",
      "     class_2     0.0000    0.0000    0.0000       176\n",
      "     class_3     0.0000    0.0000    0.0000        22\n",
      "\n",
      "    accuracy                         0.5902       615\n",
      "   macro avg     0.1476    0.2500    0.1856       615\n",
      "weighted avg     0.3484    0.5902    0.4382       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECGNet(\n",
    "    num_classes=4,\n",
    "    n_fft=512,\n",
    "    hop_length=256,\n",
    "    conv1_padding=1,\n",
    "    conv2_padding=1,\n",
    "    conv1_kernel=3,\n",
    "    conv2_kernel=3,\n",
    "    lstm_num_layers=1,\n",
    "    conv1_channels=32,\n",
    "    conv2_channels=32,\n",
    "    lst_hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    signal_length=X_train.shape[1],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, augment_data=True, device=device)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  # Modo evaluación\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "            \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "    \n",
    "df.to_csv('reduced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737b27d",
   "metadata": {},
   "source": [
    "## The results are not worth of keep trying this approach, even thou the data was compressed, because the f1 score passed from .75 to .18 with the same model architechture. \n",
    "\n",
    "## Now lossless compression will be implemented and the same model retrained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbd35bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "[✓] Loaded X_train with 6179 sequences\n",
      "[✓] Loaded y_train with shape (6179, 1)\n",
      "Splitting and padding...\n",
      "Batch shapes → X: torch.Size([32, 18286]), lengths: torch.Size([32]), y: torch.Size([32, 1])\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "def lossless_import():\n",
    "\n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train = load_train_data()\n",
    "    \n",
    "    use_compressed = False\n",
    "    \n",
    "    if use_compressed:\n",
    "        \n",
    "        compressed_path = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"compressed_data.bin\")\n",
    "        write_compressed_file(compressed_path, X_train)\n",
    "\n",
    "        X_train_compressed = read_compressed_file(\"compressed_data.bin\")\n",
    "        X_tensor_list = [torch.tensor(x, dtype=torch.float32) for x in X_train_compressed]\n",
    "        X_train_pad = pad_sequence(X_tensor_list, batch_first=True)\n",
    "        lengths_train_final = torch.tensor([len(x) for x in X_train_compressed])\n",
    "        y_train_tensor = torch.tensor(y_train.iloc[:, 0].values, dtype=torch.long)\n",
    "\n",
    "    durations = np.array([len(x) / 300 for x in X_train])\n",
    "\n",
    "    print(\"Splitting and padding...\")\n",
    "\n",
    "    X_train, X_val, lengths_train, lengths_val, y_train, y_val = (\n",
    "        stratified_split_pad_torch(X_train, y_train)\n",
    "    )     \n",
    "    \n",
    "    train_dataset = EDGCDataset(X_train_pad, lengths_train_final, y_train_tensor) if use_compressed else EDGCDataset(X_train, lengths_train, y_train)\n",
    "    val_dataset = EDGCDataset(X_val, lengths_val, y_val)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=32, shuffle=True\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "    for X_batch, lengths_batch, y_batch in train_loader:\n",
    "        print(\n",
    "            f\"Batch shapes → X: {X_batch.shape}, lengths: {lengths_batch.shape}, y: {y_batch.shape}\"\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print(\"Data preparation completed.\")\n",
    "\n",
    "    return train_loader, val_loader, X_train.shape[1]\n",
    "\n",
    "train_loader, val_loader,signal_size = lossless_import()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eeee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9704 - Train F1: 0.2264 - Val Loss: 0.9144 - Val F1: 0.2458\n",
      "Epoch 2/50 - Train Loss: 0.8822 - Train F1: 0.2819 - Val Loss: 0.8260 - Val F1: 0.3067\n",
      "Epoch 3/50 - Train Loss: 0.8013 - Train F1: 0.3451 - Val Loss: 0.7681 - Val F1: 0.4003\n",
      "Epoch 4/50 - Train Loss: 0.7516 - Train F1: 0.4584 - Val Loss: 0.7370 - Val F1: 0.4514\n",
      "Epoch 5/50 - Train Loss: 0.7097 - Train F1: 0.5024 - Val Loss: 0.6628 - Val F1: 0.5381\n",
      "Epoch 6/50 - Train Loss: 0.6634 - Train F1: 0.5848 - Val Loss: 0.6593 - Val F1: 0.5166\n",
      "Epoch 7/50 - Train Loss: 0.6330 - Train F1: 0.6234 - Val Loss: 0.6426 - Val F1: 0.5377\n",
      "Epoch 8/50 - Train Loss: 0.6113 - Train F1: 0.6418 - Val Loss: 0.5795 - Val F1: 0.6326\n",
      "Epoch 9/50 - Train Loss: 0.5816 - Train F1: 0.6545 - Val Loss: 0.5632 - Val F1: 0.6827\n",
      "Epoch 10/50 - Train Loss: 0.5780 - Train F1: 0.6680 - Val Loss: 0.5808 - Val F1: 0.6562\n",
      "Epoch 11/50 - Train Loss: 0.5610 - Train F1: 0.6800 - Val Loss: 0.5593 - Val F1: 0.6680\n",
      "Epoch 12/50 - Train Loss: 0.5412 - Train F1: 0.7104 - Val Loss: 0.5594 - Val F1: 0.6615\n",
      "Epoch 13/50 - Train Loss: 0.5433 - Train F1: 0.7080 - Val Loss: 0.5445 - Val F1: 0.6923\n",
      "Epoch 14/50 - Train Loss: 0.5294 - Train F1: 0.7024 - Val Loss: 0.5566 - Val F1: 0.6287\n",
      "Epoch 15/50 - Train Loss: 0.5243 - Train F1: 0.7249 - Val Loss: 0.5249 - Val F1: 0.7060\n",
      "Epoch 16/50 - Train Loss: 0.5147 - Train F1: 0.7205 - Val Loss: 0.5248 - Val F1: 0.6689\n",
      "Epoch 17/50 - Train Loss: 0.5084 - Train F1: 0.7185 - Val Loss: 0.5248 - Val F1: 0.7072\n",
      "Epoch 18/50 - Train Loss: 0.5045 - Train F1: 0.7368 - Val Loss: 0.5147 - Val F1: 0.7005\n",
      "Epoch 19/50 - Train Loss: 0.5047 - Train F1: 0.7417 - Val Loss: 0.5288 - Val F1: 0.6777\n",
      "Epoch 20/50 - Train Loss: 0.4977 - Train F1: 0.7405 - Val Loss: 0.5192 - Val F1: 0.6541\n",
      "Epoch 21/50 - Train Loss: 0.4913 - Train F1: 0.7487 - Val Loss: 0.5039 - Val F1: 0.6806\n",
      "Epoch 22/50 - Train Loss: 0.4755 - Train F1: 0.7541 - Val Loss: 0.5275 - Val F1: 0.6387\n",
      "Epoch 23/50 - Train Loss: 0.4761 - Train F1: 0.7515 - Val Loss: 0.5180 - Val F1: 0.6696\n",
      "Epoch 24/50 - Train Loss: 0.4686 - Train F1: 0.7553 - Val Loss: 0.5156 - Val F1: 0.6747\n",
      "Epoch 25/50 - Train Loss: 0.4611 - Train F1: 0.7683 - Val Loss: 0.5071 - Val F1: 0.6933\n",
      "Epoch 26/50 - Train Loss: 0.4613 - Train F1: 0.7647 - Val Loss: 0.5144 - Val F1: 0.6658\n",
      "\n",
      "Early stopping triggered at epoch 26\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.8399    0.9394    0.8869       363\n",
      "     class_1     0.8621    0.4630    0.6024        54\n",
      "     class_2     0.7202    0.6875    0.7035       176\n",
      "     class_3     0.6667    0.3636    0.4706        22\n",
      "\n",
      "    accuracy                         0.8049       615\n",
      "   macro avg     0.7722    0.6134    0.6658       615\n",
      "weighted avg     0.8014    0.8049    0.7945       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECGNet(\n",
    "    num_classes=4,\n",
    "    n_fft=512,\n",
    "    hop_length=256,\n",
    "    conv1_padding=1,\n",
    "    conv2_padding=1,\n",
    "    conv1_kernel=3,\n",
    "    conv2_kernel=3,\n",
    "    lstm_num_layers=1,\n",
    "    conv1_channels=32,\n",
    "    conv2_channels=32,\n",
    "    lst_hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    signal_length=X_train.shape[1],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, augment_data=True, device=device)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  # Modo evaluación\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "            \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('reduced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e1dae",
   "metadata": {},
   "source": [
    "## The same model we used with the augmented data, and which showed a f1 score of .75 now gives .69, but with a data set half the size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff142c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Loaded X_train with 6179 sequences\n",
      "[✓] Loaded y_train with shape (6179, 1)\n"
     ]
    }
   ],
   "source": [
    "compressed_path = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"lossless_data.bin\")\n",
    "X_train, y_train = load_train_data()\n",
    "write_compressed_file(compressed_path, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c1f40",
   "metadata": {},
   "source": [
    "## Lets use augment data for improving performance of the model training. Now with TCN classifier. \n",
    "##### This is the loop for choosing the best model parameter combination with augmented data. The function hyperparameter_search has the parameter augmented_data, which initialize a different data pipeline for loading and processing the data. When this parameter is False, it only takes the raw matrix X_train. However, for augmented_data = True, the pipeline implements time stretch, time_shift, add noise, amplitude scale amd random crop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78551ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9341 | Train F1: 0.2667 | Val Loss: 0.8246 | Val F1: 0.3874\n",
      "Epoch 2/5 | Train Loss: 0.7934 | Train F1: 0.3967 | Val Loss: 0.7065 | Val F1: 0.4543\n",
      "Epoch 3/5 | Train Loss: 0.7096 | Train F1: 0.4864 | Val Loss: 0.6723 | Val F1: 0.5496\n",
      "Epoch 4/5 | Train Loss: 0.6553 | Train F1: 0.5339 | Val Loss: 0.6016 | Val F1: 0.6278\n",
      "Epoch 5/5 | Train Loss: 0.6234 | Train F1: 0.5856 | Val Loss: 0.6080 | Val F1: 0.6162\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9239 | Train F1: 0.2720 | Val Loss: 0.8040 | Val F1: 0.3220\n",
      "Epoch 2/5 | Train Loss: 0.7996 | Train F1: 0.4021 | Val Loss: 0.7239 | Val F1: 0.4130\n",
      "Epoch 3/5 | Train Loss: 0.7336 | Train F1: 0.4836 | Val Loss: 0.6804 | Val F1: 0.4839\n",
      "Epoch 4/5 | Train Loss: 0.6871 | Train F1: 0.5301 | Val Loss: 0.6770 | Val F1: 0.4842\n",
      "Epoch 5/5 | Train Loss: 0.6300 | Train F1: 0.5822 | Val Loss: 0.6284 | Val F1: 0.5835\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9244 | Train F1: 0.2742 | Val Loss: 0.8530 | Val F1: 0.3459\n",
      "Epoch 2/5 | Train Loss: 0.8157 | Train F1: 0.3817 | Val Loss: 0.7535 | Val F1: 0.5241\n",
      "Epoch 3/5 | Train Loss: 0.7473 | Train F1: 0.4591 | Val Loss: 0.7428 | Val F1: 0.4567\n",
      "Epoch 4/5 | Train Loss: 0.6611 | Train F1: 0.5476 | Val Loss: 0.6472 | Val F1: 0.5783\n",
      "Epoch 5/5 | Train Loss: 0.6290 | Train F1: 0.5788 | Val Loss: 0.5963 | Val F1: 0.6277\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9211 | Train F1: 0.2833 | Val Loss: 0.8305 | Val F1: 0.3116\n",
      "Epoch 2/5 | Train Loss: 0.8280 | Train F1: 0.3848 | Val Loss: 0.7826 | Val F1: 0.3326\n",
      "Epoch 3/5 | Train Loss: 0.7560 | Train F1: 0.4749 | Val Loss: 0.7012 | Val F1: 0.4682\n",
      "Epoch 4/5 | Train Loss: 0.6812 | Train F1: 0.5566 | Val Loss: 0.6955 | Val F1: 0.4621\n",
      "Epoch 5/5 | Train Loss: 0.6421 | Train F1: 0.5635 | Val Loss: 0.6444 | Val F1: 0.5591\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9335 | Train F1: 0.2793 | Val Loss: 0.8327 | Val F1: 0.3091\n",
      "Epoch 2/5 | Train Loss: 0.8098 | Train F1: 0.3799 | Val Loss: 0.7355 | Val F1: 0.4112\n",
      "Epoch 3/5 | Train Loss: 0.7440 | Train F1: 0.4684 | Val Loss: 0.7087 | Val F1: 0.4640\n",
      "Epoch 4/5 | Train Loss: 0.6936 | Train F1: 0.4902 | Val Loss: 0.6456 | Val F1: 0.4534\n",
      "Epoch 5/5 | Train Loss: 0.6571 | Train F1: 0.5443 | Val Loss: 0.7058 | Val F1: 0.4488\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9438 | Train F1: 0.2647 | Val Loss: 0.8714 | Val F1: 0.3051\n",
      "Epoch 2/5 | Train Loss: 0.8385 | Train F1: 0.3730 | Val Loss: 0.8892 | Val F1: 0.3345\n",
      "Epoch 3/5 | Train Loss: 0.7560 | Train F1: 0.4510 | Val Loss: 0.7152 | Val F1: 0.4937\n",
      "Epoch 4/5 | Train Loss: 0.7000 | Train F1: 0.5016 | Val Loss: 0.6830 | Val F1: 0.5716\n",
      "Epoch 5/5 | Train Loss: 0.6657 | Train F1: 0.5494 | Val Loss: 0.6245 | Val F1: 0.5107\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9487 | Train F1: 0.2620 | Val Loss: 0.8689 | Val F1: 0.3180\n",
      "Epoch 2/5 | Train Loss: 0.8164 | Train F1: 0.3641 | Val Loss: 0.7621 | Val F1: 0.4148\n",
      "Epoch 3/5 | Train Loss: 0.7348 | Train F1: 0.4636 | Val Loss: 0.7733 | Val F1: 0.3640\n",
      "Epoch 4/5 | Train Loss: 0.6955 | Train F1: 0.5020 | Val Loss: 0.7064 | Val F1: 0.4337\n",
      "Epoch 5/5 | Train Loss: 0.6525 | Train F1: 0.5549 | Val Loss: 0.6460 | Val F1: 0.5471\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9479 | Train F1: 0.2549 | Val Loss: 0.8360 | Val F1: 0.2966\n",
      "Epoch 2/5 | Train Loss: 0.8216 | Train F1: 0.3912 | Val Loss: 0.7499 | Val F1: 0.4273\n",
      "Epoch 3/5 | Train Loss: 0.7589 | Train F1: 0.4700 | Val Loss: 0.7425 | Val F1: 0.6033\n",
      "Epoch 4/5 | Train Loss: 0.6949 | Train F1: 0.5385 | Val Loss: 0.6442 | Val F1: 0.5090\n",
      "Epoch 5/5 | Train Loss: 0.6433 | Train F1: 0.5678 | Val Loss: 0.6137 | Val F1: 0.6384\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9188 | Train F1: 0.2817 | Val Loss: 0.8224 | Val F1: 0.3171\n",
      "Epoch 2/5 | Train Loss: 0.8093 | Train F1: 0.3955 | Val Loss: 0.7287 | Val F1: 0.3979\n",
      "Epoch 3/5 | Train Loss: 0.7654 | Train F1: 0.4398 | Val Loss: 0.6928 | Val F1: 0.5150\n",
      "Epoch 4/5 | Train Loss: 0.7051 | Train F1: 0.4878 | Val Loss: 0.6547 | Val F1: 0.5371\n",
      "Epoch 5/5 | Train Loss: 0.6656 | Train F1: 0.5407 | Val Loss: 0.6753 | Val F1: 0.5966\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9124 | Train F1: 0.2799 | Val Loss: 0.8001 | Val F1: 0.3291\n",
      "Epoch 2/5 | Train Loss: 0.8119 | Train F1: 0.3840 | Val Loss: 0.7779 | Val F1: 0.3527\n",
      "Epoch 3/5 | Train Loss: 0.7505 | Train F1: 0.4729 | Val Loss: 0.7387 | Val F1: 0.4297\n",
      "Epoch 4/5 | Train Loss: 0.6890 | Train F1: 0.5199 | Val Loss: 0.7044 | Val F1: 0.4489\n",
      "Epoch 5/5 | Train Loss: 0.6676 | Train F1: 0.5443 | Val Loss: 0.7231 | Val F1: 0.5709\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9459 | Train F1: 0.2623 | Val Loss: 0.8209 | Val F1: 0.3047\n",
      "Epoch 2/5 | Train Loss: 0.8059 | Train F1: 0.3796 | Val Loss: 0.8575 | Val F1: 0.4350\n",
      "Epoch 3/5 | Train Loss: 0.7474 | Train F1: 0.4622 | Val Loss: 0.7115 | Val F1: 0.4810\n",
      "Epoch 4/5 | Train Loss: 0.7172 | Train F1: 0.5049 | Val Loss: 0.6919 | Val F1: 0.5650\n",
      "Epoch 5/5 | Train Loss: 0.6908 | Train F1: 0.5254 | Val Loss: 0.6950 | Val F1: 0.4887\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9489 | Train F1: 0.2636 | Val Loss: 0.8637 | Val F1: 0.3248\n",
      "Epoch 2/5 | Train Loss: 0.8233 | Train F1: 0.3938 | Val Loss: 0.7620 | Val F1: 0.4717\n",
      "Epoch 3/5 | Train Loss: 0.7530 | Train F1: 0.4861 | Val Loss: 0.6950 | Val F1: 0.4210\n",
      "Epoch 4/5 | Train Loss: 0.7076 | Train F1: 0.5285 | Val Loss: 0.6875 | Val F1: 0.4739\n",
      "Epoch 5/5 | Train Loss: 0.6463 | Train F1: 0.5507 | Val Loss: 0.6944 | Val F1: 0.5316\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9415 | Train F1: 0.2950 | Val Loss: 0.8218 | Val F1: 0.3525\n",
      "Epoch 2/5 | Train Loss: 0.8029 | Train F1: 0.3921 | Val Loss: 0.8645 | Val F1: 0.4353\n",
      "Epoch 3/5 | Train Loss: 0.7225 | Train F1: 0.4714 | Val Loss: 0.6564 | Val F1: 0.4510\n",
      "Epoch 4/5 | Train Loss: 0.6600 | Train F1: 0.5417 | Val Loss: 0.6253 | Val F1: 0.5993\n",
      "Epoch 5/5 | Train Loss: 0.6254 | Train F1: 0.5874 | Val Loss: 0.5888 | Val F1: 0.6004\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9311 | Train F1: 0.3115 | Val Loss: 0.8214 | Val F1: 0.3277\n",
      "Epoch 2/5 | Train Loss: 0.8019 | Train F1: 0.3895 | Val Loss: 0.7321 | Val F1: 0.4468\n",
      "Epoch 3/5 | Train Loss: 0.7252 | Train F1: 0.4832 | Val Loss: 0.6655 | Val F1: 0.5031\n",
      "Epoch 4/5 | Train Loss: 0.6599 | Train F1: 0.5253 | Val Loss: 0.6168 | Val F1: 0.4996\n",
      "Epoch 5/5 | Train Loss: 0.6322 | Train F1: 0.5816 | Val Loss: 0.6310 | Val F1: 0.4562\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9889 | Train F1: 0.2766 | Val Loss: 0.8496 | Val F1: 0.3088\n",
      "Epoch 2/5 | Train Loss: 0.8340 | Train F1: 0.3547 | Val Loss: 0.7750 | Val F1: 0.3600\n",
      "Epoch 3/5 | Train Loss: 0.7792 | Train F1: 0.4525 | Val Loss: 0.8082 | Val F1: 0.4404\n",
      "Epoch 4/5 | Train Loss: 0.7209 | Train F1: 0.5023 | Val Loss: 0.6988 | Val F1: 0.4259\n",
      "Epoch 5/5 | Train Loss: 0.6632 | Train F1: 0.5355 | Val Loss: 0.6746 | Val F1: 0.6458\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9651 | Train F1: 0.2805 | Val Loss: 0.8184 | Val F1: 0.3376\n",
      "Epoch 2/5 | Train Loss: 0.7919 | Train F1: 0.4109 | Val Loss: 0.7747 | Val F1: 0.4583\n",
      "Epoch 3/5 | Train Loss: 0.7141 | Train F1: 0.4945 | Val Loss: 0.6733 | Val F1: 0.4800\n",
      "Epoch 4/5 | Train Loss: 0.6513 | Train F1: 0.5563 | Val Loss: 0.6922 | Val F1: 0.4717\n",
      "Epoch 5/5 | Train Loss: 0.6085 | Train F1: 0.6095 | Val Loss: 0.6696 | Val F1: 0.5996\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9659 | Train F1: 0.2819 | Val Loss: 0.8310 | Val F1: 0.3529\n",
      "Epoch 2/5 | Train Loss: 0.7995 | Train F1: 0.4141 | Val Loss: 0.7505 | Val F1: 0.5611\n",
      "Epoch 3/5 | Train Loss: 0.7308 | Train F1: 0.4918 | Val Loss: 0.7372 | Val F1: 0.3510\n",
      "Epoch 4/5 | Train Loss: 0.6789 | Train F1: 0.5042 | Val Loss: 0.6813 | Val F1: 0.6039\n",
      "Epoch 5/5 | Train Loss: 0.6507 | Train F1: 0.5658 | Val Loss: 0.7113 | Val F1: 0.4978\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9451 | Train F1: 0.3044 | Val Loss: 0.8237 | Val F1: 0.3202\n",
      "Epoch 2/5 | Train Loss: 0.8040 | Train F1: 0.4129 | Val Loss: 0.7726 | Val F1: 0.4676\n",
      "Epoch 3/5 | Train Loss: 0.7315 | Train F1: 0.4888 | Val Loss: 0.6671 | Val F1: 0.4649\n",
      "Epoch 4/5 | Train Loss: 0.6803 | Train F1: 0.5294 | Val Loss: 0.6201 | Val F1: 0.5424\n",
      "Epoch 5/5 | Train Loss: 0.6659 | Train F1: 0.5476 | Val Loss: 0.7297 | Val F1: 0.4930\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9484 | Train F1: 0.2847 | Val Loss: 0.8865 | Val F1: 0.3020\n",
      "Epoch 2/5 | Train Loss: 0.8187 | Train F1: 0.3939 | Val Loss: 0.8444 | Val F1: 0.4086\n",
      "Epoch 3/5 | Train Loss: 0.7764 | Train F1: 0.4436 | Val Loss: 0.7220 | Val F1: 0.4582\n",
      "Epoch 4/5 | Train Loss: 0.6883 | Train F1: 0.5073 | Val Loss: 0.7637 | Val F1: 0.4120\n",
      "Epoch 5/5 | Train Loss: 0.6561 | Train F1: 0.5479 | Val Loss: 0.6421 | Val F1: 0.5347\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9596 | Train F1: 0.2794 | Val Loss: 0.8258 | Val F1: 0.3812\n",
      "Epoch 2/5 | Train Loss: 0.8089 | Train F1: 0.3805 | Val Loss: 0.7771 | Val F1: 0.4204\n",
      "Epoch 3/5 | Train Loss: 0.7801 | Train F1: 0.4311 | Val Loss: 0.7110 | Val F1: 0.4060\n",
      "Epoch 4/5 | Train Loss: 0.7010 | Train F1: 0.5123 | Val Loss: 0.6771 | Val F1: 0.3828\n",
      "Epoch 5/5 | Train Loss: 0.6647 | Train F1: 0.5465 | Val Loss: 0.6670 | Val F1: 0.5752\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9505 | Train F1: 0.2813 | Val Loss: 0.9207 | Val F1: 0.2196\n",
      "Epoch 2/5 | Train Loss: 0.8208 | Train F1: 0.3835 | Val Loss: 0.7896 | Val F1: 0.3839\n",
      "Epoch 3/5 | Train Loss: 0.7569 | Train F1: 0.4452 | Val Loss: 0.6662 | Val F1: 0.4614\n",
      "Epoch 4/5 | Train Loss: 0.7138 | Train F1: 0.4832 | Val Loss: 0.6663 | Val F1: 0.5182\n",
      "Epoch 5/5 | Train Loss: 0.6687 | Train F1: 0.5396 | Val Loss: 0.6735 | Val F1: 0.4855\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9895 | Train F1: 0.2858 | Val Loss: 0.9410 | Val F1: 0.3923\n",
      "Epoch 2/5 | Train Loss: 0.8437 | Train F1: 0.3521 | Val Loss: 0.8888 | Val F1: 0.3499\n",
      "Epoch 3/5 | Train Loss: 0.7595 | Train F1: 0.4400 | Val Loss: 0.7359 | Val F1: 0.4442\n",
      "Epoch 4/5 | Train Loss: 0.7017 | Train F1: 0.4735 | Val Loss: 0.6824 | Val F1: 0.4796\n",
      "Epoch 5/5 | Train Loss: 0.6656 | Train F1: 0.5394 | Val Loss: 0.7082 | Val F1: 0.4560\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9388 | Train F1: 0.2997 | Val Loss: 0.8148 | Val F1: 0.3940\n",
      "Epoch 2/5 | Train Loss: 0.7828 | Train F1: 0.4223 | Val Loss: 0.7682 | Val F1: 0.4925\n",
      "Epoch 3/5 | Train Loss: 0.7598 | Train F1: 0.4681 | Val Loss: 0.7351 | Val F1: 0.3611\n",
      "Epoch 4/5 | Train Loss: 0.7115 | Train F1: 0.4942 | Val Loss: 0.6820 | Val F1: 0.5231\n",
      "Epoch 5/5 | Train Loss: 0.6855 | Train F1: 0.5367 | Val Loss: 0.6307 | Val F1: 0.5841\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 1.0050 | Train F1: 0.2575 | Val Loss: 0.9399 | Val F1: 0.2265\n",
      "Epoch 2/5 | Train Loss: 0.8421 | Train F1: 0.3606 | Val Loss: 0.8176 | Val F1: 0.4402\n",
      "Epoch 3/5 | Train Loss: 0.7753 | Train F1: 0.4434 | Val Loss: 0.7975 | Val F1: 0.3840\n",
      "Epoch 4/5 | Train Loss: 0.7112 | Train F1: 0.5154 | Val Loss: 0.6408 | Val F1: 0.5266\n",
      "Epoch 5/5 | Train Loss: 0.6590 | Train F1: 0.5524 | Val Loss: 0.6267 | Val F1: 0.5267\n",
      "\n",
      "🏆 Top 3 configurations:\n",
      "\n",
      "#1 - Val F1: 0.6458\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 5\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [128, 128, 128, 128]\n",
      "  dropout: 0.1\n",
      "  num_levels: 3\n",
      "Train F1: 0.5355 | Val Loss: 0.6746\n",
      "\n",
      "#2 - Val F1: 0.6384\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 5\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [64, 128, 128, 128]\n",
      "  dropout: 0.2\n",
      "  num_levels: 4\n",
      "Train F1: 0.5678 | Val Loss: 0.6137\n",
      "\n",
      "#3 - Val F1: 0.6278\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 3\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [64, 128, 128, 128]\n",
      "  dropout: 0.1\n",
      "  num_levels: 3\n",
      "Train F1: 0.5339 | Val Loss: 0.6016\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # Configuraciones donde len(hidden_channels) == num_levels\n",
    "    \n",
    "        'hidden_channels': [[64,128,128,128],[128,128,128,128]],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'kernel_size': [3, 5],\n",
    "    'num_levels': [3,4]\n",
    "}\n",
    "\n",
    "\n",
    "fixed = {\n",
    "    \"num_classes\": 4,\n",
    "    \"n_fft\": 256,\n",
    "    \"hop_length\": 128,\n",
    "    \"kernel_size\": 3,\n",
    "    \"learning_rate\" : .001,\n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    TCN_STFT_Classifier,\n",
    "    param_grid,\n",
    "    fixed,\n",
    "    device=device,\n",
    "    epochs=5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    augmented_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859870a",
   "metadata": {},
   "source": [
    "## We get our best parameter selection for the model trained with data augmentation for TCN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5028a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\\venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9650 - Train F1: 0.2736 - Val Loss: 0.8357 - Val F1: 0.4054\n",
      "Epoch 2/50 - Train Loss: 0.8224 - Train F1: 0.3944 - Val Loss: 0.7376 - Val F1: 0.4935\n",
      "Epoch 3/50 - Train Loss: 0.7503 - Train F1: 0.4549 - Val Loss: 0.6875 - Val F1: 0.4449\n",
      "Epoch 4/50 - Train Loss: 0.6898 - Train F1: 0.5139 - Val Loss: 0.6970 - Val F1: 0.5146\n",
      "Epoch 5/50 - Train Loss: 0.6438 - Train F1: 0.5835 - Val Loss: 0.6304 - Val F1: 0.5737\n",
      "Epoch 6/50 - Train Loss: 0.6245 - Train F1: 0.5828 - Val Loss: 0.6476 - Val F1: 0.4823\n",
      "Epoch 7/50 - Train Loss: 0.5934 - Train F1: 0.6272 - Val Loss: 0.5826 - Val F1: 0.6639\n",
      "Epoch 8/50 - Train Loss: 0.5788 - Train F1: 0.6337 - Val Loss: 0.6112 - Val F1: 0.6324\n",
      "Epoch 9/50 - Train Loss: 0.5585 - Train F1: 0.6468 - Val Loss: 0.5903 - Val F1: 0.6286\n",
      "Epoch 10/50 - Train Loss: 0.5346 - Train F1: 0.6806 - Val Loss: 0.5894 - Val F1: 0.6740\n",
      "Epoch 11/50 - Train Loss: 0.5312 - Train F1: 0.6820 - Val Loss: 0.5735 - Val F1: 0.6734\n",
      "Epoch 12/50 - Train Loss: 0.5180 - Train F1: 0.6884 - Val Loss: 0.5590 - Val F1: 0.7127\n",
      "Epoch 13/50 - Train Loss: 0.5032 - Train F1: 0.6848 - Val Loss: 0.5546 - Val F1: 0.7028\n",
      "Epoch 14/50 - Train Loss: 0.4955 - Train F1: 0.7035 - Val Loss: 0.5243 - Val F1: 0.6890\n",
      "Epoch 15/50 - Train Loss: 0.4848 - Train F1: 0.7126 - Val Loss: 0.5463 - Val F1: 0.6751\n",
      "Epoch 16/50 - Train Loss: 0.4866 - Train F1: 0.7198 - Val Loss: 0.5505 - Val F1: 0.7062\n",
      "Epoch 17/50 - Train Loss: 0.4698 - Train F1: 0.7300 - Val Loss: 0.5276 - Val F1: 0.6948\n",
      "Epoch 18/50 - Train Loss: 0.4673 - Train F1: 0.7290 - Val Loss: 0.6008 - Val F1: 0.6505\n",
      "Epoch 19/50 - Train Loss: 0.4624 - Train F1: 0.7409 - Val Loss: 0.5143 - Val F1: 0.7262\n",
      "Epoch 20/50 - Train Loss: 0.4484 - Train F1: 0.7533 - Val Loss: 0.5415 - Val F1: 0.7119\n",
      "Epoch 21/50 - Train Loss: 0.4449 - Train F1: 0.7468 - Val Loss: 0.5226 - Val F1: 0.7174\n",
      "Epoch 22/50 - Train Loss: 0.4447 - Train F1: 0.7441 - Val Loss: 0.5022 - Val F1: 0.7161\n",
      "Epoch 23/50 - Train Loss: 0.4256 - Train F1: 0.7537 - Val Loss: 0.5363 - Val F1: 0.7184\n",
      "Epoch 24/50 - Train Loss: 0.4253 - Train F1: 0.7654 - Val Loss: 0.4980 - Val F1: 0.7325\n",
      "Epoch 25/50 - Train Loss: 0.4199 - Train F1: 0.7798 - Val Loss: 0.5193 - Val F1: 0.7216\n",
      "Epoch 26/50 - Train Loss: 0.4210 - Train F1: 0.7713 - Val Loss: 0.4679 - Val F1: 0.7406\n",
      "Epoch 27/50 - Train Loss: 0.4169 - Train F1: 0.7732 - Val Loss: 0.5066 - Val F1: 0.7003\n",
      "Epoch 28/50 - Train Loss: 0.3955 - Train F1: 0.8001 - Val Loss: 0.4792 - Val F1: 0.7325\n",
      "Epoch 29/50 - Train Loss: 0.3884 - Train F1: 0.7882 - Val Loss: 0.5063 - Val F1: 0.7282\n",
      "Epoch 30/50 - Train Loss: 0.3935 - Train F1: 0.7773 - Val Loss: 0.5338 - Val F1: 0.7323\n",
      "Epoch 31/50 - Train Loss: 0.3825 - Train F1: 0.7920 - Val Loss: 0.5125 - Val F1: 0.7205\n",
      "\n",
      "Early stopping triggered at epoch 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.8529    0.9587    0.9027       363\n",
      "     class_1     0.8378    0.5741    0.6813        54\n",
      "     class_2     0.7817    0.6307    0.6981       176\n",
      "     class_3     0.5357    0.6818    0.6000        22\n",
      "\n",
      "    accuracy                         0.8211       615\n",
      "   macro avg     0.7520    0.7113    0.7205       615\n",
      "weighted avg     0.8199    0.8211    0.8139       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "from src.models.model_trainer import Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = TCN_STFT_Classifier(\n",
    "    num_classes=4,\n",
    "    hop_length = 128,\n",
    "    n_fft = 256,\n",
    "    kernel_size = 5, \n",
    "    hidden_channels=  [128, 128, 128, 128],\n",
    "    dropout = 0.1,\n",
    "    num_levels = 3,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, device=device, augment_data = True)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  # Modo evaluación\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "            \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "    \n",
    "df.to_csv('augment.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
