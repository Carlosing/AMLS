# ECG Signal Classification using Deep Learning

## 1. Introduction

Electrocardiogram (ECG) signal classification is a critical task in healthcare, enabling the detection of arrhythmias and other cardiac abnormalities. This project focuses on classifying **univariate ECG time-series signals** into four clinically relevant categories:
- **Normal rhythm**
- **Atrial Fibrillation (AF)**
- **Other arrhythmias**
- **Noisy signals**

The dataset consists of **variable-length ECG recordings sampled at 300 Hz**, which introduces several challenges such as class imbalance, heterogeneous signal lengths, and noise. To address these issues, we designed an end-to-end deep learning pipeline covering:
- Exploratory data analysis
- Data preprocessing and normalization
- Model design and training
- Hyperparameter tuning
- Data augmentation
- Data reduction (compression)
- Final evaluation and test prediction generation

### Models

Two distinct deep learning architectures were implemented and evaluated:
1. **CNNâ€“LSTM Hybrid Model**
   - Uses **Short-Time Fourier Transform (STFT)** for timeâ€“frequency representation.
   - Combines convolutional feature extraction with temporal modeling.
2. **Temporal Convolutional Network (TCN)**
   - Employs dilated causal convolutions for long-range temporal dependencies.

Model evaluation was performed using **stratified train/validation splits**, and **F1-score** was chosen as the primary metric due to class imbalance. Hyperparameters were optimized via grid search.

### Additional Experiments

Beyond model design, the project also investigates:
- **Data augmentation** (e.g., time stretching, noise injection) to improve generalization
- **Data reduction** through lossless and lossy compression to reduce storage requirements

### Key Findings

- The **CNNâ€“LSTM model outperformed the TCN**, achieving an F1-score of **0.748** compared to **0.702**.
- Data augmentation showed **marginal improvement** on validation performance.
- **50% lossless compression** preserved classification performance with minimal F1-score degradation.

### Deliverables

The repository includes:
- Modular and reproducible code
- Detailed model evaluation and analysis
- Test-set predictions for three scenarios:
  - `base.csv` â€” baseline model
  - `augment.csv` â€” trained with augmented data
  - `reduced.csv` â€” trained with compressed data

Although the achieved performance is not yet optimal, the results highlight promising directions for future work, such as **ensemble learning** or **transformer-based architectures**.

---

## 2. How to Run the Code

### 2.1 Folder Structure

```
AMLS/
â”œâ”€â”€ main.py                     # Main controller script
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ base.csv
â”œâ”€â”€ preparation_1/
â”‚   â”œâ”€â”€ data_preparation.py
â”‚   â””â”€â”€ Exploring.ipynb
â”œâ”€â”€ Modelling_2/
â”‚   â”œâ”€â”€ model1_training.py
â”‚   â”œâ”€â”€ model1_evaluate.py
â”‚   â”œâ”€â”€ model2_training.py
â”‚   â”œâ”€â”€ model2_evaluate.py
â”‚   â””â”€â”€ Modelling.ipynb
â”œâ”€â”€ Data_augmentation_3/
â”‚   â”œâ”€â”€ model1_augmented.py
â”‚   â”œâ”€â”€ model2_augmented.py
â”‚   â””â”€â”€ Data_augmentation.ipynb
â”œâ”€â”€ Data_reduction_4/
â”‚   â”œâ”€â”€ data_lossless.py
â”‚   â””â”€â”€ Reduction.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ X_train.bin
â”‚   â”‚   â”œâ”€â”€ y_train.csv
â”‚   â”‚   â””â”€â”€ X_test.bin
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ compressed.bin
â””â”€â”€ src/
    â”œâ”€â”€ augmentation/
    â”‚   â””â”€â”€ signal_augmentations.py
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ load_data.py
    â”‚   â”œâ”€â”€ lossless_compression.py
    â”‚   â”œâ”€â”€ lossy_compression.py
    â”‚   â””â”€â”€ stratified_split.py
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ model_1/
    â”‚   â”‚   â”œâ”€â”€ architecture.py
    â”‚   â”‚   â””â”€â”€ config.yaml
    â”‚   â””â”€â”€ model_2/
    â”‚       â”œâ”€â”€ architecture.py
    â”‚       â””â”€â”€ config.yaml
    â””â”€â”€ hyperparameter_tunning/
        â”œâ”€â”€ grid_search.py
        â”œâ”€â”€ bayesian_opt.py
        â””â”€â”€ model_trainer.py
```

---

### 2.2 Comprehensive Execution Guide

2.2.1 Environment Setup

**1. Create a virtual environment**

```bash
python -m venv ecg-env
```

Activate it:

```bash
# Linux / macOS
source ecg-env/bin/activate

# Windows
ecg-env\Scripts\activate
```

**2. Install dependencies**

```bash
pip install -r requirements.txt
```

Key dependencies include **PyTorch**, **scikit-learn**, **pandas**, and **Jupyter**.

**3. Data preparation**

Download the dataset from **TU-Cloud** and place the files in:

```
data/raw/
â”œâ”€â”€ X_train.bin   # Training signals
â”œâ”€â”€ y_train.csv   # Training labels
â””â”€â”€ X_test.bin    # Test signals (no labels)
```

---

### 2.2.2 Data Loading and Preparation

```bash
python main.py --prepare
```

**What it does:**
- Parses binary ECG signals
- Applies Z-score normalization per signal
- Performs a **90% / 10% stratified trainâ€“validation split**
- Pads shorter signals with zeros
- Creates PyTorch-compatible dataloaders

---

### 2.2.3 Hyperparameter Tuning

```bash
python main.py --hyperparameter_model1
python main.py --hyperparameter_model2
```

**What it does:**
- Runs grid search for the selected model
- Prints F1-score per epoch and parameter combination
- Returns the **top 3 hyperparameter configurations**
- Outputs a confusion matrix for the best model

---

### 2.2.4 Model Training and Evaluation

```bash
python main.py --evaluate_model1
python main.py --evaluate_model2
```

**What it does:**
- Trains the selected model for **50 epochs** using optimal hyperparameters
- Prints epoch-wise F1-scores
- Outputs:
  - Confusion matrix
  - Accuracy, precision, recall, and F1-score per class
- Saves test predictions to:

```
base.csv
```

---

### 2.2.5 Training with Augmented Data

```bash
python main.py --model1_augmented
python main.py --model2_augmented
```

**What it does:**
- Applies signal-level augmentations (e.g., noise injection, time stretching)
- Retrains models using the same hyperparameters
- Outputs evaluation metrics and confusion matrix
- Saves test predictions to:

```
augment.csv
```

---

### 2.2.6 Training with Compressed Data

```bash
python main.py --lossless_model1
python main.py --lossless_model2
```

**What it does:**
- Applies **50% lossless compression** to the training dataset
- Retrains models on compressed signals
- Evaluates performance and reports metrics
- Saves test predictions to:

```
reduced.csv
```

---

ðŸ“Œ **Note:** All experiments are controlled via `main.py`, ensuring reproducibility and consistency across training, augmentation, and compression scenarios.

---
