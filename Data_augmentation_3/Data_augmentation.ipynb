{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch as torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e09000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Project at: c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\n"
     ]
    }
   ],
   "source": [
    "# Add the project root path if not already present\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # move up one level from notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "# Visual confirmation\n",
    "print(\"[✓] Project at:\", PROJECT_ROOT)\n",
    "\n",
    "from src.data.load_data import load_train_data, EDGCDataset, load_test_data, EDGCTestDataset\n",
    "from src.data.stratified_split import stratified_split_pad_torch, pad_test_torch\n",
    "from src.models.model_trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c79eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Loaded X_train with 6179 sequences\n",
      "[✓] Loaded y_train with shape (6179, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_train_data()\n",
    "\n",
    "durations = np.array([len(x) / 300 for x in X_train])\n",
    "\n",
    "cls_count = y_train[0].groupby(y_train[0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0376f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5564, 18286]) torch.Size([5564, 1])\n",
      "[✓] Loaded X_test with 2649 sequences\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, lengths_train, lengths_val, y_train, y_val = stratified_split_pad_torch(\n",
    "    X_train, y_train\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = EDGCDataset(X_train, lengths_train, y_train)\n",
    "val_dataset = EDGCDataset(X_val, lengths_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "X_test = load_test_data()\n",
    "\n",
    "X_test, lengths_test = pad_test_torch(X_test)\n",
    "\n",
    "test_dataset = EDGCTestDataset(X_test, lengths_test)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42b209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.model_1 import ECGNet\n",
    "from src.models.hyperparamter_tunning import hyperparameter_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d0490b",
   "metadata": {},
   "source": [
    "## Lets use augment data for improving performance of the model training. First with CNN-LSTM Model Architecture. \n",
    "\n",
    "##### This is the loop for choosing the best model parameter combination with augmented data. The function hyperparameter_search has the parameter augmented_data, which initialize a different data pipeline for loading and processing the data. When this parameter is False, it only takes the raw matrix X_train. However, for augmented_data = True, the pipeline implements time stretch, time_shift, add noise, amplitude scale amd random crop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236e30da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0292 | Train F1: 0.1876 | Val Loss: 1.0027 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0173 | Train F1: 0.1856 | Val Loss: 1.0097 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0164 | Train F1: 0.1853 | Val Loss: 1.0110 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0150 | Train F1: 0.1856 | Val Loss: 1.0059 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0131 | Train F1: 0.1853 | Val Loss: 1.0083 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0150 | Train F1: 0.1853 | Val Loss: 1.0042 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0169 | Train F1: 0.1853 | Val Loss: 1.0052 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0277 | Train F1: 0.1863 | Val Loss: 1.0042 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0171 | Train F1: 0.1853 | Val Loss: 1.0048 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0238 | Train F1: 0.1853 | Val Loss: 1.0107 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0166 | Train F1: 0.1874 | Val Loss: 1.0214 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0188 | Train F1: 0.1852 | Val Loss: 1.0063 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0180 | Train F1: 0.1853 | Val Loss: 1.0085 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0159 | Train F1: 0.1853 | Val Loss: 1.0076 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0497 | Train F1: 0.1963 | Val Loss: 1.0035 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0376 | Train F1: 0.1904 | Val Loss: 1.0125 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0240 | Train F1: 0.1876 | Val Loss: 1.0094 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0276 | Train F1: 0.1887 | Val Loss: 1.0080 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0234 | Train F1: 0.1915 | Val Loss: 1.0138 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0259 | Train F1: 0.1876 | Val Loss: 1.0123 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0234 | Train F1: 0.1859 | Val Loss: 1.0109 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0212 | Train F1: 0.2025 | Val Loss: 0.9834 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9848 | Train F1: 0.1866 | Val Loss: 0.9539 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9692 | Train F1: 0.2012 | Val Loss: 0.9546 | Val F1: 0.2086\n",
      "Epoch 4/7 | Train Loss: 0.9456 | Train F1: 0.2330 | Val Loss: 0.9144 | Val F1: 0.2064\n",
      "Epoch 5/7 | Train Loss: 0.9045 | Train F1: 0.2561 | Val Loss: 0.8391 | Val F1: 0.2765\n",
      "Epoch 6/7 | Train Loss: 0.8504 | Train F1: 0.3145 | Val Loss: 0.7955 | Val F1: 0.3349\n",
      "Epoch 7/7 | Train Loss: 0.8092 | Train F1: 0.3721 | Val Loss: 0.7819 | Val F1: 0.3594\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0383 | Train F1: 0.2019 | Val Loss: 0.9799 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9957 | Train F1: 0.1912 | Val Loss: 0.9796 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9787 | Train F1: 0.1965 | Val Loss: 0.9571 | Val F1: 0.2071\n",
      "Epoch 4/7 | Train Loss: 0.9691 | Train F1: 0.2168 | Val Loss: 0.9225 | Val F1: 0.2435\n",
      "Epoch 5/7 | Train Loss: 0.9514 | Train F1: 0.2385 | Val Loss: 0.8925 | Val F1: 0.2541\n",
      "Epoch 6/7 | Train Loss: 0.9137 | Train F1: 0.2689 | Val Loss: 0.8671 | Val F1: 0.2647\n",
      "Epoch 7/7 | Train Loss: 0.8928 | Train F1: 0.2773 | Val Loss: 0.8481 | Val F1: 0.2838\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.1368 | Train F1: 0.2244 | Val Loss: 0.9843 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0404 | Train F1: 0.1973 | Val Loss: 1.0009 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0270 | Train F1: 0.1871 | Val Loss: 1.0007 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0214 | Train F1: 0.1856 | Val Loss: 0.9981 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0187 | Train F1: 0.1859 | Val Loss: 0.9981 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0152 | Train F1: 0.1856 | Val Loss: 0.9901 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0174 | Train F1: 0.1856 | Val Loss: 0.9986 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0650 | Train F1: 0.2077 | Val Loss: 0.9796 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9808 | Train F1: 0.1901 | Val Loss: 0.9549 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9676 | Train F1: 0.2011 | Val Loss: 0.9198 | Val F1: 0.2074\n",
      "Epoch 4/7 | Train Loss: 0.9482 | Train F1: 0.2190 | Val Loss: 0.8989 | Val F1: 0.2491\n",
      "Epoch 5/7 | Train Loss: 0.9102 | Train F1: 0.2587 | Val Loss: 0.8469 | Val F1: 0.3133\n",
      "Epoch 6/7 | Train Loss: 0.8771 | Train F1: 0.3012 | Val Loss: 0.8125 | Val F1: 0.2937\n",
      "Epoch 7/7 | Train Loss: 0.8482 | Train F1: 0.3232 | Val Loss: 0.8028 | Val F1: 0.3581\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0677 | Train F1: 0.2004 | Val Loss: 0.9967 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9965 | Train F1: 0.1975 | Val Loss: 0.9620 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9799 | Train F1: 0.2042 | Val Loss: 0.9259 | Val F1: 0.2037\n",
      "Epoch 4/7 | Train Loss: 0.9552 | Train F1: 0.2305 | Val Loss: 0.8911 | Val F1: 0.2288\n",
      "Epoch 5/7 | Train Loss: 0.9279 | Train F1: 0.2453 | Val Loss: 0.8495 | Val F1: 0.2544\n",
      "Epoch 6/7 | Train Loss: 0.8773 | Train F1: 0.2835 | Val Loss: 0.8238 | Val F1: 0.2720\n",
      "Epoch 7/7 | Train Loss: 0.8509 | Train F1: 0.3080 | Val Loss: 0.7861 | Val F1: 0.3267\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 32, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.1564 | Train F1: 0.2127 | Val Loss: 0.9995 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0465 | Train F1: 0.1979 | Val Loss: 0.9909 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0333 | Train F1: 0.1916 | Val Loss: 0.9958 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0247 | Train F1: 0.1875 | Val Loss: 0.9897 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0187 | Train F1: 0.1879 | Val Loss: 0.9936 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0166 | Train F1: 0.1904 | Val Loss: 0.9815 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0083 | Train F1: 0.1859 | Val Loss: 0.9825 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0246 | Train F1: 0.1866 | Val Loss: 1.0197 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0184 | Train F1: 0.1853 | Val Loss: 1.0221 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0207 | Train F1: 0.1853 | Val Loss: 1.0079 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0192 | Train F1: 0.1852 | Val Loss: 1.0147 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0234 | Train F1: 0.1858 | Val Loss: 1.0271 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0213 | Train F1: 0.1860 | Val Loss: 1.0184 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0267 | Train F1: 0.1856 | Val Loss: 1.0136 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0282 | Train F1: 0.1890 | Val Loss: 1.0228 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0209 | Train F1: 0.1852 | Val Loss: 1.0155 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0235 | Train F1: 0.1892 | Val Loss: 1.0080 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0231 | Train F1: 0.1879 | Val Loss: 1.0049 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0202 | Train F1: 0.1864 | Val Loss: 1.0087 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0247 | Train F1: 0.1852 | Val Loss: 1.0041 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0219 | Train F1: 0.1859 | Val Loss: 1.0158 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0491 | Train F1: 0.1990 | Val Loss: 1.0368 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0387 | Train F1: 0.1933 | Val Loss: 1.0051 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0322 | Train F1: 0.1935 | Val Loss: 1.0293 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0285 | Train F1: 0.1997 | Val Loss: 1.0057 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0292 | Train F1: 0.1926 | Val Loss: 1.0060 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0294 | Train F1: 0.1978 | Val Loss: 1.0137 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0280 | Train F1: 0.1880 | Val Loss: 1.0060 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9963 | Train F1: 0.1915 | Val Loss: 0.9657 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9815 | Train F1: 0.1865 | Val Loss: 0.9662 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9741 | Train F1: 0.1873 | Val Loss: 0.9474 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9348 | Train F1: 0.2066 | Val Loss: 0.8691 | Val F1: 0.2280\n",
      "Epoch 5/7 | Train Loss: 0.8680 | Train F1: 0.3039 | Val Loss: 0.8032 | Val F1: 0.3180\n",
      "Epoch 6/7 | Train Loss: 0.8223 | Train F1: 0.3903 | Val Loss: 0.7644 | Val F1: 0.3557\n",
      "Epoch 7/7 | Train Loss: 0.7968 | Train F1: 0.4183 | Val Loss: 0.7484 | Val F1: 0.3413\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0174 | Train F1: 0.1870 | Val Loss: 0.9702 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9903 | Train F1: 0.1880 | Val Loss: 0.9528 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9698 | Train F1: 0.1903 | Val Loss: 0.9386 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9214 | Train F1: 0.2313 | Val Loss: 0.8557 | Val F1: 0.2562\n",
      "Epoch 5/7 | Train Loss: 0.8802 | Train F1: 0.2779 | Val Loss: 0.8209 | Val F1: 0.2828\n",
      "Epoch 6/7 | Train Loss: 0.8404 | Train F1: 0.3556 | Val Loss: 0.7734 | Val F1: 0.4398\n",
      "Epoch 7/7 | Train Loss: 0.7984 | Train F1: 0.4203 | Val Loss: 0.7622 | Val F1: 0.4251\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0788 | Train F1: 0.2029 | Val Loss: 1.0028 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0229 | Train F1: 0.1903 | Val Loss: 0.9988 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0135 | Train F1: 0.1859 | Val Loss: 0.9972 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0126 | Train F1: 0.1852 | Val Loss: 0.9880 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0077 | Train F1: 0.1855 | Val Loss: 0.9884 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0076 | Train F1: 0.1852 | Val Loss: 0.9895 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0041 | Train F1: 0.1853 | Val Loss: 0.9830 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0026 | Train F1: 0.2121 | Val Loss: 0.9429 | Val F1: 0.2166\n",
      "Epoch 2/7 | Train Loss: 0.9526 | Train F1: 0.2191 | Val Loss: 0.9072 | Val F1: 0.2475\n",
      "Epoch 3/7 | Train Loss: 0.9003 | Train F1: 0.2715 | Val Loss: 0.8705 | Val F1: 0.2304\n",
      "Epoch 4/7 | Train Loss: 0.8399 | Train F1: 0.3316 | Val Loss: 0.7988 | Val F1: 0.2727\n",
      "Epoch 5/7 | Train Loss: 0.8026 | Train F1: 0.4199 | Val Loss: 0.7497 | Val F1: 0.5582\n",
      "Epoch 6/7 | Train Loss: 0.7651 | Train F1: 0.4842 | Val Loss: 0.7307 | Val F1: 0.4651\n",
      "Epoch 7/7 | Train Loss: 0.7333 | Train F1: 0.5315 | Val Loss: 0.6918 | Val F1: 0.5650\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0105 | Train F1: 0.2078 | Val Loss: 0.9578 | Val F1: 0.1886\n",
      "Epoch 2/7 | Train Loss: 0.9799 | Train F1: 0.1971 | Val Loss: 0.9451 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9689 | Train F1: 0.2029 | Val Loss: 0.9463 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 0.9566 | Train F1: 0.2105 | Val Loss: 0.9204 | Val F1: 0.1997\n",
      "Epoch 5/7 | Train Loss: 0.9438 | Train F1: 0.2457 | Val Loss: 0.8969 | Val F1: 0.2797\n",
      "Epoch 6/7 | Train Loss: 0.9198 | Train F1: 0.2627 | Val Loss: 0.8695 | Val F1: 0.2472\n",
      "Epoch 7/7 | Train Loss: 0.8927 | Train F1: 0.2851 | Val Loss: 0.8360 | Val F1: 0.2869\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 64, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0898 | Train F1: 0.2254 | Val Loss: 1.0097 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0288 | Train F1: 0.1933 | Val Loss: 0.9891 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0194 | Train F1: 0.1875 | Val Loss: 0.9934 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0110 | Train F1: 0.1882 | Val Loss: 0.9859 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0136 | Train F1: 0.1852 | Val Loss: 0.9802 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0011 | Train F1: 0.1885 | Val Loss: 0.9725 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0073 | Train F1: 0.1883 | Val Loss: 0.9632 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 1.0378 | Train F1: 0.1959 | Val Loss: 1.0051 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0324 | Train F1: 0.1951 | Val Loss: 1.0423 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0308 | Train F1: 0.2040 | Val Loss: 1.0303 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0388 | Train F1: 0.1962 | Val Loss: 1.0132 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0385 | Train F1: 0.1934 | Val Loss: 1.0107 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0299 | Train F1: 0.2010 | Val Loss: 1.0514 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0373 | Train F1: 0.1976 | Val Loss: 1.0242 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0512 | Train F1: 0.2040 | Val Loss: 1.0078 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0361 | Train F1: 0.1975 | Val Loss: 1.0268 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0441 | Train F1: 0.2079 | Val Loss: 1.0385 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0336 | Train F1: 0.1992 | Val Loss: 1.0064 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0396 | Train F1: 0.1998 | Val Loss: 1.0073 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0440 | Train F1: 0.2092 | Val Loss: 1.0111 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0387 | Train F1: 0.1928 | Val Loss: 1.0422 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.01, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0715 | Train F1: 0.2197 | Val Loss: 1.0253 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0642 | Train F1: 0.2140 | Val Loss: 1.0229 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0580 | Train F1: 0.2090 | Val Loss: 1.0024 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0531 | Train F1: 0.2103 | Val Loss: 1.0267 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 1.0479 | Train F1: 0.2116 | Val Loss: 0.9984 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 1.0567 | Train F1: 0.2099 | Val Loss: 1.0103 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 1.0518 | Train F1: 0.2050 | Val Loss: 1.0143 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9682 | Train F1: 0.2214 | Val Loss: 0.9204 | Val F1: 0.3024\n",
      "Epoch 2/7 | Train Loss: 0.8493 | Train F1: 0.3266 | Val Loss: 0.7780 | Val F1: 0.3450\n",
      "Epoch 3/7 | Train Loss: 0.7550 | Train F1: 0.4590 | Val Loss: 0.6991 | Val F1: 0.5252\n",
      "Epoch 4/7 | Train Loss: 0.7067 | Train F1: 0.5389 | Val Loss: 0.6651 | Val F1: 0.6386\n",
      "Epoch 5/7 | Train Loss: 0.6483 | Train F1: 0.6135 | Val Loss: 0.6331 | Val F1: 0.6519\n",
      "Epoch 6/7 | Train Loss: 0.6324 | Train F1: 0.6326 | Val Loss: 0.5921 | Val F1: 0.6809\n",
      "Epoch 7/7 | Train Loss: 0.6044 | Train F1: 0.6622 | Val Loss: 0.5770 | Val F1: 0.6984\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 1.0035 | Train F1: 0.1902 | Val Loss: 0.9738 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9737 | Train F1: 0.1902 | Val Loss: 0.9442 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 0.9583 | Train F1: 0.2019 | Val Loss: 0.8996 | Val F1: 0.2122\n",
      "Epoch 4/7 | Train Loss: 0.9183 | Train F1: 0.2556 | Val Loss: 0.8308 | Val F1: 0.3074\n",
      "Epoch 5/7 | Train Loss: 0.8607 | Train F1: 0.3095 | Val Loss: 0.8073 | Val F1: 0.3212\n",
      "Epoch 6/7 | Train Loss: 0.8206 | Train F1: 0.3400 | Val Loss: 0.7618 | Val F1: 0.3551\n",
      "Epoch 7/7 | Train Loss: 0.7820 | Train F1: 0.4139 | Val Loss: 0.7339 | Val F1: 0.3807\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.001, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0478 | Train F1: 0.1991 | Val Loss: 0.9918 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0089 | Train F1: 0.1951 | Val Loss: 0.9849 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0082 | Train F1: 0.1875 | Val Loss: 0.9808 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0005 | Train F1: 0.1861 | Val Loss: 0.9688 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 0.9998 | Train F1: 0.1867 | Val Loss: 0.9759 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 0.9998 | Train F1: 0.1870 | Val Loss: 0.9705 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 0.9957 | Train F1: 0.1856 | Val Loss: 0.9639 | Val F1: 0.1856\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.1}\n",
      "Epoch 1/7 | Train Loss: 0.9996 | Train F1: 0.1966 | Val Loss: 0.9524 | Val F1: 0.1883\n",
      "Epoch 2/7 | Train Loss: 0.9630 | Train F1: 0.2045 | Val Loss: 0.9400 | Val F1: 0.1886\n",
      "Epoch 3/7 | Train Loss: 0.9438 | Train F1: 0.2154 | Val Loss: 0.9037 | Val F1: 0.2425\n",
      "Epoch 4/7 | Train Loss: 0.9174 | Train F1: 0.2470 | Val Loss: 0.8661 | Val F1: 0.3134\n",
      "Epoch 5/7 | Train Loss: 0.8673 | Train F1: 0.3028 | Val Loss: 0.8205 | Val F1: 0.2918\n",
      "Epoch 6/7 | Train Loss: 0.8223 | Train F1: 0.3701 | Val Loss: 0.7728 | Val F1: 0.4452\n",
      "Epoch 7/7 | Train Loss: 0.7896 | Train F1: 0.4204 | Val Loss: 0.7489 | Val F1: 0.4590\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.2}\n",
      "Epoch 1/7 | Train Loss: 0.9985 | Train F1: 0.2041 | Val Loss: 0.9346 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 0.9687 | Train F1: 0.2018 | Val Loss: 0.9139 | Val F1: 0.2064\n",
      "Epoch 3/7 | Train Loss: 0.9405 | Train F1: 0.2440 | Val Loss: 0.8779 | Val F1: 0.2570\n",
      "Epoch 4/7 | Train Loss: 0.9155 | Train F1: 0.2715 | Val Loss: 0.8330 | Val F1: 0.2916\n",
      "Epoch 5/7 | Train Loss: 0.8512 | Train F1: 0.3267 | Val Loss: 0.7686 | Val F1: 0.4398\n",
      "Epoch 6/7 | Train Loss: 0.8049 | Train F1: 0.4172 | Val Loss: 0.7432 | Val F1: 0.4482\n",
      "Epoch 7/7 | Train Loss: 0.7771 | Train F1: 0.4733 | Val Loss: 0.7193 | Val F1: 0.4759\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'signal_length': 18286, 'n_fft': 512, 'hop_length': 256, 'conv1_padding': 1, 'conv2_padding': 1, 'conv1_kernel': 3, 'conv2_kernel': 3, 'lstm_num_layers': 1, 'conv1_channels': 32, 'conv2_channels': 32, 'lst_hidden_size': 128, 'learning_rate': 0.0005, 'dropout': 0.5}\n",
      "Epoch 1/7 | Train Loss: 1.0565 | Train F1: 0.2176 | Val Loss: 0.9968 | Val F1: 0.1856\n",
      "Epoch 2/7 | Train Loss: 1.0101 | Train F1: 0.1946 | Val Loss: 0.9696 | Val F1: 0.1856\n",
      "Epoch 3/7 | Train Loss: 1.0013 | Train F1: 0.1899 | Val Loss: 0.9693 | Val F1: 0.1856\n",
      "Epoch 4/7 | Train Loss: 1.0035 | Train F1: 0.1888 | Val Loss: 0.9753 | Val F1: 0.1856\n",
      "Epoch 5/7 | Train Loss: 0.9936 | Train F1: 0.1897 | Val Loss: 0.9578 | Val F1: 0.1856\n",
      "Epoch 6/7 | Train Loss: 0.9890 | Train F1: 0.1910 | Val Loss: 0.9498 | Val F1: 0.1856\n",
      "Epoch 7/7 | Train Loss: 0.9862 | Train F1: 0.1945 | Val Loss: 0.9462 | Val F1: 0.1856\n",
      "\n",
      "🏆 Top 3 configurations:\n",
      "\n",
      "#1 - Val F1: 0.6984\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 128\n",
      "  learning_rate: 0.001\n",
      "  dropout: 0.1\n",
      "Train F1: 0.6622 | Val Loss: 0.5770\n",
      "\n",
      "#2 - Val F1: 0.5650\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 64\n",
      "  learning_rate: 0.0005\n",
      "  dropout: 0.1\n",
      "Train F1: 0.5315 | Val Loss: 0.6918\n",
      "\n",
      "#3 - Val F1: 0.4759\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  signal_length: 18286\n",
      "  n_fft: 512\n",
      "  hop_length: 256\n",
      "  conv1_padding: 1\n",
      "  conv2_padding: 1\n",
      "  conv1_kernel: 3\n",
      "  conv2_kernel: 3\n",
      "  lstm_num_layers: 1\n",
      "  conv1_channels: 32\n",
      "  conv2_channels: 32\n",
      "  lst_hidden_size: 128\n",
      "  learning_rate: 0.0005\n",
      "  dropout: 0.2\n",
      "Train F1: 0.4733 | Val Loss: 0.7193\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "param_grid = {\n",
    "    \"lst_hidden_size\": [32, 64, 128],\n",
    "    \n",
    "    \"learning_rate\": [.01, 0.001, 0.0005],\n",
    "    \n",
    "    \"dropout\": [0.1, 0.2, .5],\n",
    "}\n",
    "fixed = {\n",
    "    \"num_classes\": 4,\n",
    "    \"signal_length\": X_train.shape[1],\n",
    "    \"n_fft\": 512,\n",
    "    \"hop_length\": 256,\n",
    "    \"conv1_padding\": 1,\n",
    "    \"conv2_padding\": 1,\n",
    "    \"conv1_kernel\": 3,\n",
    "    \"conv2_kernel\": 3,\n",
    "    \"lstm_num_layers\": 1,\n",
    "    \"conv1_channels\": 32,\n",
    "    \"conv2_channels\": 32\n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    ECGNet,\n",
    "    param_grid,\n",
    "    fixed,\n",
    "    device=device,\n",
    "    epochs=7,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    augmented_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95374640",
   "metadata": {},
   "source": [
    "## We get our best parameter selection for the model trained with data augmentation for CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459218b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9759 - Train F1: 0.2054 - Val Loss: 0.9094 - Val F1: 0.2011\n",
      "Epoch 2/50 - Train Loss: 0.8775 - Train F1: 0.2868 - Val Loss: 0.7988 - Val F1: 0.2897\n",
      "Epoch 3/50 - Train Loss: 0.7906 - Train F1: 0.3879 - Val Loss: 0.7371 - Val F1: 0.5188\n",
      "Epoch 4/50 - Train Loss: 0.7445 - Train F1: 0.4803 - Val Loss: 0.7088 - Val F1: 0.4938\n",
      "Epoch 5/50 - Train Loss: 0.7256 - Train F1: 0.5288 - Val Loss: 0.6912 - Val F1: 0.5521\n",
      "Epoch 6/50 - Train Loss: 0.6868 - Train F1: 0.5708 - Val Loss: 0.7062 - Val F1: 0.5532\n",
      "Epoch 7/50 - Train Loss: 0.6688 - Train F1: 0.6013 - Val Loss: 0.6405 - Val F1: 0.5611\n",
      "Epoch 8/50 - Train Loss: 0.6454 - Train F1: 0.6093 - Val Loss: 0.6347 - Val F1: 0.5831\n",
      "Epoch 9/50 - Train Loss: 0.6270 - Train F1: 0.6138 - Val Loss: 0.6132 - Val F1: 0.6343\n",
      "Epoch 10/50 - Train Loss: 0.6102 - Train F1: 0.6456 - Val Loss: 0.6282 - Val F1: 0.6356\n",
      "Epoch 11/50 - Train Loss: 0.5871 - Train F1: 0.6744 - Val Loss: 0.5763 - Val F1: 0.6977\n",
      "Epoch 12/50 - Train Loss: 0.5794 - Train F1: 0.6738 - Val Loss: 0.5838 - Val F1: 0.6347\n",
      "Epoch 13/50 - Train Loss: 0.5597 - Train F1: 0.6923 - Val Loss: 0.5613 - Val F1: 0.7116\n",
      "Epoch 14/50 - Train Loss: 0.5543 - Train F1: 0.6910 - Val Loss: 0.5843 - Val F1: 0.7018\n",
      "Epoch 15/50 - Train Loss: 0.5439 - Train F1: 0.7029 - Val Loss: 0.5603 - Val F1: 0.7250\n",
      "Epoch 16/50 - Train Loss: 0.5326 - Train F1: 0.7212 - Val Loss: 0.5491 - Val F1: 0.7160\n",
      "Epoch 17/50 - Train Loss: 0.5254 - Train F1: 0.7152 - Val Loss: 0.5528 - Val F1: 0.7264\n",
      "Epoch 18/50 - Train Loss: 0.5085 - Train F1: 0.7328 - Val Loss: 0.5364 - Val F1: 0.7234\n",
      "Epoch 19/50 - Train Loss: 0.5068 - Train F1: 0.7237 - Val Loss: 0.5358 - Val F1: 0.7061\n",
      "Epoch 20/50 - Train Loss: 0.5023 - Train F1: 0.7367 - Val Loss: 0.5244 - Val F1: 0.7278\n",
      "Epoch 21/50 - Train Loss: 0.4990 - Train F1: 0.7383 - Val Loss: 0.5117 - Val F1: 0.7392\n",
      "Epoch 22/50 - Train Loss: 0.4911 - Train F1: 0.7381 - Val Loss: 0.5147 - Val F1: 0.7371\n",
      "Epoch 23/50 - Train Loss: 0.4806 - Train F1: 0.7491 - Val Loss: 0.5308 - Val F1: 0.7379\n",
      "Epoch 24/50 - Train Loss: 0.4700 - Train F1: 0.7492 - Val Loss: 0.5167 - Val F1: 0.7458\n",
      "Epoch 25/50 - Train Loss: 0.4662 - Train F1: 0.7631 - Val Loss: 0.5101 - Val F1: 0.7599\n",
      "Epoch 26/50 - Train Loss: 0.4556 - Train F1: 0.7715 - Val Loss: 0.5009 - Val F1: 0.7626\n",
      "Epoch 27/50 - Train Loss: 0.4486 - Train F1: 0.7700 - Val Loss: 0.5058 - Val F1: 0.7776\n",
      "Epoch 28/50 - Train Loss: 0.4499 - Train F1: 0.7700 - Val Loss: 0.5078 - Val F1: 0.7421\n",
      "Epoch 29/50 - Train Loss: 0.4475 - Train F1: 0.7787 - Val Loss: 0.5132 - Val F1: 0.7452\n",
      "Epoch 30/50 - Train Loss: 0.4370 - Train F1: 0.7921 - Val Loss: 0.5046 - Val F1: 0.7460\n",
      "Epoch 31/50 - Train Loss: 0.4323 - Train F1: 0.7885 - Val Loss: 0.5302 - Val F1: 0.7390\n",
      "\n",
      "Early stopping triggered at epoch 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.8165    0.9559    0.8807       363\n",
      "     class_1     0.8718    0.6296    0.7312        54\n",
      "     class_2     0.7939    0.5909    0.6775       176\n",
      "     class_3     0.7000    0.6364    0.6667        22\n",
      "\n",
      "    accuracy                         0.8114       615\n",
      "   macro avg     0.7955    0.7032    0.7390       615\n",
      "weighted avg     0.8107    0.8114    0.8018       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECGNet(\n",
    "    num_classes=4,\n",
    "    n_fft=512,\n",
    "    hop_length=256,\n",
    "    conv1_padding=1,\n",
    "    conv2_padding=1,\n",
    "    conv1_kernel=3,\n",
    "    conv2_kernel=3,\n",
    "    lstm_num_layers=1,\n",
    "    conv1_channels=32,\n",
    "    conv2_channels=32,\n",
    "    lst_hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    signal_length=X_train.shape[1],\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, augment_data=True, device=device)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  # Modo evaluación\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "            \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "    \n",
    "df.to_csv('augment.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c1f40",
   "metadata": {},
   "source": [
    "## Lets use augment data for improving performance of the model training. Now with TCN classifier. \n",
    "##### This is the loop for choosing the best model parameter combination with augmented data. The function hyperparameter_search has the parameter augmented_data, which initialize a different data pipeline for loading and processing the data. When this parameter is False, it only takes the raw matrix X_train. However, for augmented_data = True, the pipeline implements time stretch, time_shift, add noise, amplitude scale amd random crop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a78551ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9341 | Train F1: 0.2667 | Val Loss: 0.8246 | Val F1: 0.3874\n",
      "Epoch 2/5 | Train Loss: 0.7934 | Train F1: 0.3967 | Val Loss: 0.7065 | Val F1: 0.4543\n",
      "Epoch 3/5 | Train Loss: 0.7096 | Train F1: 0.4864 | Val Loss: 0.6723 | Val F1: 0.5496\n",
      "Epoch 4/5 | Train Loss: 0.6553 | Train F1: 0.5339 | Val Loss: 0.6016 | Val F1: 0.6278\n",
      "Epoch 5/5 | Train Loss: 0.6234 | Train F1: 0.5856 | Val Loss: 0.6080 | Val F1: 0.6162\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9239 | Train F1: 0.2720 | Val Loss: 0.8040 | Val F1: 0.3220\n",
      "Epoch 2/5 | Train Loss: 0.7996 | Train F1: 0.4021 | Val Loss: 0.7239 | Val F1: 0.4130\n",
      "Epoch 3/5 | Train Loss: 0.7336 | Train F1: 0.4836 | Val Loss: 0.6804 | Val F1: 0.4839\n",
      "Epoch 4/5 | Train Loss: 0.6871 | Train F1: 0.5301 | Val Loss: 0.6770 | Val F1: 0.4842\n",
      "Epoch 5/5 | Train Loss: 0.6300 | Train F1: 0.5822 | Val Loss: 0.6284 | Val F1: 0.5835\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9244 | Train F1: 0.2742 | Val Loss: 0.8530 | Val F1: 0.3459\n",
      "Epoch 2/5 | Train Loss: 0.8157 | Train F1: 0.3817 | Val Loss: 0.7535 | Val F1: 0.5241\n",
      "Epoch 3/5 | Train Loss: 0.7473 | Train F1: 0.4591 | Val Loss: 0.7428 | Val F1: 0.4567\n",
      "Epoch 4/5 | Train Loss: 0.6611 | Train F1: 0.5476 | Val Loss: 0.6472 | Val F1: 0.5783\n",
      "Epoch 5/5 | Train Loss: 0.6290 | Train F1: 0.5788 | Val Loss: 0.5963 | Val F1: 0.6277\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9211 | Train F1: 0.2833 | Val Loss: 0.8305 | Val F1: 0.3116\n",
      "Epoch 2/5 | Train Loss: 0.8280 | Train F1: 0.3848 | Val Loss: 0.7826 | Val F1: 0.3326\n",
      "Epoch 3/5 | Train Loss: 0.7560 | Train F1: 0.4749 | Val Loss: 0.7012 | Val F1: 0.4682\n",
      "Epoch 4/5 | Train Loss: 0.6812 | Train F1: 0.5566 | Val Loss: 0.6955 | Val F1: 0.4621\n",
      "Epoch 5/5 | Train Loss: 0.6421 | Train F1: 0.5635 | Val Loss: 0.6444 | Val F1: 0.5591\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9335 | Train F1: 0.2793 | Val Loss: 0.8327 | Val F1: 0.3091\n",
      "Epoch 2/5 | Train Loss: 0.8098 | Train F1: 0.3799 | Val Loss: 0.7355 | Val F1: 0.4112\n",
      "Epoch 3/5 | Train Loss: 0.7440 | Train F1: 0.4684 | Val Loss: 0.7087 | Val F1: 0.4640\n",
      "Epoch 4/5 | Train Loss: 0.6936 | Train F1: 0.4902 | Val Loss: 0.6456 | Val F1: 0.4534\n",
      "Epoch 5/5 | Train Loss: 0.6571 | Train F1: 0.5443 | Val Loss: 0.7058 | Val F1: 0.4488\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9438 | Train F1: 0.2647 | Val Loss: 0.8714 | Val F1: 0.3051\n",
      "Epoch 2/5 | Train Loss: 0.8385 | Train F1: 0.3730 | Val Loss: 0.8892 | Val F1: 0.3345\n",
      "Epoch 3/5 | Train Loss: 0.7560 | Train F1: 0.4510 | Val Loss: 0.7152 | Val F1: 0.4937\n",
      "Epoch 4/5 | Train Loss: 0.7000 | Train F1: 0.5016 | Val Loss: 0.6830 | Val F1: 0.5716\n",
      "Epoch 5/5 | Train Loss: 0.6657 | Train F1: 0.5494 | Val Loss: 0.6245 | Val F1: 0.5107\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9487 | Train F1: 0.2620 | Val Loss: 0.8689 | Val F1: 0.3180\n",
      "Epoch 2/5 | Train Loss: 0.8164 | Train F1: 0.3641 | Val Loss: 0.7621 | Val F1: 0.4148\n",
      "Epoch 3/5 | Train Loss: 0.7348 | Train F1: 0.4636 | Val Loss: 0.7733 | Val F1: 0.3640\n",
      "Epoch 4/5 | Train Loss: 0.6955 | Train F1: 0.5020 | Val Loss: 0.7064 | Val F1: 0.4337\n",
      "Epoch 5/5 | Train Loss: 0.6525 | Train F1: 0.5549 | Val Loss: 0.6460 | Val F1: 0.5471\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9479 | Train F1: 0.2549 | Val Loss: 0.8360 | Val F1: 0.2966\n",
      "Epoch 2/5 | Train Loss: 0.8216 | Train F1: 0.3912 | Val Loss: 0.7499 | Val F1: 0.4273\n",
      "Epoch 3/5 | Train Loss: 0.7589 | Train F1: 0.4700 | Val Loss: 0.7425 | Val F1: 0.6033\n",
      "Epoch 4/5 | Train Loss: 0.6949 | Train F1: 0.5385 | Val Loss: 0.6442 | Val F1: 0.5090\n",
      "Epoch 5/5 | Train Loss: 0.6433 | Train F1: 0.5678 | Val Loss: 0.6137 | Val F1: 0.6384\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9188 | Train F1: 0.2817 | Val Loss: 0.8224 | Val F1: 0.3171\n",
      "Epoch 2/5 | Train Loss: 0.8093 | Train F1: 0.3955 | Val Loss: 0.7287 | Val F1: 0.3979\n",
      "Epoch 3/5 | Train Loss: 0.7654 | Train F1: 0.4398 | Val Loss: 0.6928 | Val F1: 0.5150\n",
      "Epoch 4/5 | Train Loss: 0.7051 | Train F1: 0.4878 | Val Loss: 0.6547 | Val F1: 0.5371\n",
      "Epoch 5/5 | Train Loss: 0.6656 | Train F1: 0.5407 | Val Loss: 0.6753 | Val F1: 0.5966\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9124 | Train F1: 0.2799 | Val Loss: 0.8001 | Val F1: 0.3291\n",
      "Epoch 2/5 | Train Loss: 0.8119 | Train F1: 0.3840 | Val Loss: 0.7779 | Val F1: 0.3527\n",
      "Epoch 3/5 | Train Loss: 0.7505 | Train F1: 0.4729 | Val Loss: 0.7387 | Val F1: 0.4297\n",
      "Epoch 4/5 | Train Loss: 0.6890 | Train F1: 0.5199 | Val Loss: 0.7044 | Val F1: 0.4489\n",
      "Epoch 5/5 | Train Loss: 0.6676 | Train F1: 0.5443 | Val Loss: 0.7231 | Val F1: 0.5709\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9459 | Train F1: 0.2623 | Val Loss: 0.8209 | Val F1: 0.3047\n",
      "Epoch 2/5 | Train Loss: 0.8059 | Train F1: 0.3796 | Val Loss: 0.8575 | Val F1: 0.4350\n",
      "Epoch 3/5 | Train Loss: 0.7474 | Train F1: 0.4622 | Val Loss: 0.7115 | Val F1: 0.4810\n",
      "Epoch 4/5 | Train Loss: 0.7172 | Train F1: 0.5049 | Val Loss: 0.6919 | Val F1: 0.5650\n",
      "Epoch 5/5 | Train Loss: 0.6908 | Train F1: 0.5254 | Val Loss: 0.6950 | Val F1: 0.4887\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [64, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9489 | Train F1: 0.2636 | Val Loss: 0.8637 | Val F1: 0.3248\n",
      "Epoch 2/5 | Train Loss: 0.8233 | Train F1: 0.3938 | Val Loss: 0.7620 | Val F1: 0.4717\n",
      "Epoch 3/5 | Train Loss: 0.7530 | Train F1: 0.4861 | Val Loss: 0.6950 | Val F1: 0.4210\n",
      "Epoch 4/5 | Train Loss: 0.7076 | Train F1: 0.5285 | Val Loss: 0.6875 | Val F1: 0.4739\n",
      "Epoch 5/5 | Train Loss: 0.6463 | Train F1: 0.5507 | Val Loss: 0.6944 | Val F1: 0.5316\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9415 | Train F1: 0.2950 | Val Loss: 0.8218 | Val F1: 0.3525\n",
      "Epoch 2/5 | Train Loss: 0.8029 | Train F1: 0.3921 | Val Loss: 0.8645 | Val F1: 0.4353\n",
      "Epoch 3/5 | Train Loss: 0.7225 | Train F1: 0.4714 | Val Loss: 0.6564 | Val F1: 0.4510\n",
      "Epoch 4/5 | Train Loss: 0.6600 | Train F1: 0.5417 | Val Loss: 0.6253 | Val F1: 0.5993\n",
      "Epoch 5/5 | Train Loss: 0.6254 | Train F1: 0.5874 | Val Loss: 0.5888 | Val F1: 0.6004\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9311 | Train F1: 0.3115 | Val Loss: 0.8214 | Val F1: 0.3277\n",
      "Epoch 2/5 | Train Loss: 0.8019 | Train F1: 0.3895 | Val Loss: 0.7321 | Val F1: 0.4468\n",
      "Epoch 3/5 | Train Loss: 0.7252 | Train F1: 0.4832 | Val Loss: 0.6655 | Val F1: 0.5031\n",
      "Epoch 4/5 | Train Loss: 0.6599 | Train F1: 0.5253 | Val Loss: 0.6168 | Val F1: 0.4996\n",
      "Epoch 5/5 | Train Loss: 0.6322 | Train F1: 0.5816 | Val Loss: 0.6310 | Val F1: 0.4562\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9889 | Train F1: 0.2766 | Val Loss: 0.8496 | Val F1: 0.3088\n",
      "Epoch 2/5 | Train Loss: 0.8340 | Train F1: 0.3547 | Val Loss: 0.7750 | Val F1: 0.3600\n",
      "Epoch 3/5 | Train Loss: 0.7792 | Train F1: 0.4525 | Val Loss: 0.8082 | Val F1: 0.4404\n",
      "Epoch 4/5 | Train Loss: 0.7209 | Train F1: 0.5023 | Val Loss: 0.6988 | Val F1: 0.4259\n",
      "Epoch 5/5 | Train Loss: 0.6632 | Train F1: 0.5355 | Val Loss: 0.6746 | Val F1: 0.6458\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.1, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9651 | Train F1: 0.2805 | Val Loss: 0.8184 | Val F1: 0.3376\n",
      "Epoch 2/5 | Train Loss: 0.7919 | Train F1: 0.4109 | Val Loss: 0.7747 | Val F1: 0.4583\n",
      "Epoch 3/5 | Train Loss: 0.7141 | Train F1: 0.4945 | Val Loss: 0.6733 | Val F1: 0.4800\n",
      "Epoch 4/5 | Train Loss: 0.6513 | Train F1: 0.5563 | Val Loss: 0.6922 | Val F1: 0.4717\n",
      "Epoch 5/5 | Train Loss: 0.6085 | Train F1: 0.6095 | Val Loss: 0.6696 | Val F1: 0.5996\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9659 | Train F1: 0.2819 | Val Loss: 0.8310 | Val F1: 0.3529\n",
      "Epoch 2/5 | Train Loss: 0.7995 | Train F1: 0.4141 | Val Loss: 0.7505 | Val F1: 0.5611\n",
      "Epoch 3/5 | Train Loss: 0.7308 | Train F1: 0.4918 | Val Loss: 0.7372 | Val F1: 0.3510\n",
      "Epoch 4/5 | Train Loss: 0.6789 | Train F1: 0.5042 | Val Loss: 0.6813 | Val F1: 0.6039\n",
      "Epoch 5/5 | Train Loss: 0.6507 | Train F1: 0.5658 | Val Loss: 0.7113 | Val F1: 0.4978\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9451 | Train F1: 0.3044 | Val Loss: 0.8237 | Val F1: 0.3202\n",
      "Epoch 2/5 | Train Loss: 0.8040 | Train F1: 0.4129 | Val Loss: 0.7726 | Val F1: 0.4676\n",
      "Epoch 3/5 | Train Loss: 0.7315 | Train F1: 0.4888 | Val Loss: 0.6671 | Val F1: 0.4649\n",
      "Epoch 4/5 | Train Loss: 0.6803 | Train F1: 0.5294 | Val Loss: 0.6201 | Val F1: 0.5424\n",
      "Epoch 5/5 | Train Loss: 0.6659 | Train F1: 0.5476 | Val Loss: 0.7297 | Val F1: 0.4930\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9484 | Train F1: 0.2847 | Val Loss: 0.8865 | Val F1: 0.3020\n",
      "Epoch 2/5 | Train Loss: 0.8187 | Train F1: 0.3939 | Val Loss: 0.8444 | Val F1: 0.4086\n",
      "Epoch 3/5 | Train Loss: 0.7764 | Train F1: 0.4436 | Val Loss: 0.7220 | Val F1: 0.4582\n",
      "Epoch 4/5 | Train Loss: 0.6883 | Train F1: 0.5073 | Val Loss: 0.7637 | Val F1: 0.4120\n",
      "Epoch 5/5 | Train Loss: 0.6561 | Train F1: 0.5479 | Val Loss: 0.6421 | Val F1: 0.5347\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.2, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9596 | Train F1: 0.2794 | Val Loss: 0.8258 | Val F1: 0.3812\n",
      "Epoch 2/5 | Train Loss: 0.8089 | Train F1: 0.3805 | Val Loss: 0.7771 | Val F1: 0.4204\n",
      "Epoch 3/5 | Train Loss: 0.7801 | Train F1: 0.4311 | Val Loss: 0.7110 | Val F1: 0.4060\n",
      "Epoch 4/5 | Train Loss: 0.7010 | Train F1: 0.5123 | Val Loss: 0.6771 | Val F1: 0.3828\n",
      "Epoch 5/5 | Train Loss: 0.6647 | Train F1: 0.5465 | Val Loss: 0.6670 | Val F1: 0.5752\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9505 | Train F1: 0.2813 | Val Loss: 0.9207 | Val F1: 0.2196\n",
      "Epoch 2/5 | Train Loss: 0.8208 | Train F1: 0.3835 | Val Loss: 0.7896 | Val F1: 0.3839\n",
      "Epoch 3/5 | Train Loss: 0.7569 | Train F1: 0.4452 | Val Loss: 0.6662 | Val F1: 0.4614\n",
      "Epoch 4/5 | Train Loss: 0.7138 | Train F1: 0.4832 | Val Loss: 0.6663 | Val F1: 0.5182\n",
      "Epoch 5/5 | Train Loss: 0.6687 | Train F1: 0.5396 | Val Loss: 0.6735 | Val F1: 0.4855\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 3, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 0.9895 | Train F1: 0.2858 | Val Loss: 0.9410 | Val F1: 0.3923\n",
      "Epoch 2/5 | Train Loss: 0.8437 | Train F1: 0.3521 | Val Loss: 0.8888 | Val F1: 0.3499\n",
      "Epoch 3/5 | Train Loss: 0.7595 | Train F1: 0.4400 | Val Loss: 0.7359 | Val F1: 0.4442\n",
      "Epoch 4/5 | Train Loss: 0.7017 | Train F1: 0.4735 | Val Loss: 0.6824 | Val F1: 0.4796\n",
      "Epoch 5/5 | Train Loss: 0.6656 | Train F1: 0.5394 | Val Loss: 0.7082 | Val F1: 0.4560\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 3}\n",
      "Epoch 1/5 | Train Loss: 0.9388 | Train F1: 0.2997 | Val Loss: 0.8148 | Val F1: 0.3940\n",
      "Epoch 2/5 | Train Loss: 0.7828 | Train F1: 0.4223 | Val Loss: 0.7682 | Val F1: 0.4925\n",
      "Epoch 3/5 | Train Loss: 0.7598 | Train F1: 0.4681 | Val Loss: 0.7351 | Val F1: 0.3611\n",
      "Epoch 4/5 | Train Loss: 0.7115 | Train F1: 0.4942 | Val Loss: 0.6820 | Val F1: 0.5231\n",
      "Epoch 5/5 | Train Loss: 0.6855 | Train F1: 0.5367 | Val Loss: 0.6307 | Val F1: 0.5841\n",
      "\n",
      "🔧 Training with config: {'num_classes': 4, 'n_fft': 256, 'hop_length': 128, 'kernel_size': 5, 'learning_rate': 0.001, 'hidden_channels': [128, 128, 128, 128], 'dropout': 0.3, 'num_levels': 4}\n",
      "Epoch 1/5 | Train Loss: 1.0050 | Train F1: 0.2575 | Val Loss: 0.9399 | Val F1: 0.2265\n",
      "Epoch 2/5 | Train Loss: 0.8421 | Train F1: 0.3606 | Val Loss: 0.8176 | Val F1: 0.4402\n",
      "Epoch 3/5 | Train Loss: 0.7753 | Train F1: 0.4434 | Val Loss: 0.7975 | Val F1: 0.3840\n",
      "Epoch 4/5 | Train Loss: 0.7112 | Train F1: 0.5154 | Val Loss: 0.6408 | Val F1: 0.5266\n",
      "Epoch 5/5 | Train Loss: 0.6590 | Train F1: 0.5524 | Val Loss: 0.6267 | Val F1: 0.5267\n",
      "\n",
      "🏆 Top 3 configurations:\n",
      "\n",
      "#1 - Val F1: 0.6458\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 5\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [128, 128, 128, 128]\n",
      "  dropout: 0.1\n",
      "  num_levels: 3\n",
      "Train F1: 0.5355 | Val Loss: 0.6746\n",
      "\n",
      "#2 - Val F1: 0.6384\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 5\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [64, 128, 128, 128]\n",
      "  dropout: 0.2\n",
      "  num_levels: 4\n",
      "Train F1: 0.5678 | Val Loss: 0.6137\n",
      "\n",
      "#3 - Val F1: 0.6278\n",
      "Hyperparameters:\n",
      "  num_classes: 4\n",
      "  n_fft: 256\n",
      "  hop_length: 128\n",
      "  kernel_size: 3\n",
      "  learning_rate: 0.001\n",
      "  hidden_channels: [64, 128, 128, 128]\n",
      "  dropout: 0.1\n",
      "  num_levels: 3\n",
      "Train F1: 0.5339 | Val Loss: 0.6016\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    # Configuraciones donde len(hidden_channels) == num_levels\n",
    "    \n",
    "        'hidden_channels': [[64,128,128,128],[128,128,128,128]],\n",
    "    'dropout': [0.1, 0.2, 0.3],\n",
    "    'kernel_size': [3, 5],\n",
    "    'num_levels': [3,4]\n",
    "}\n",
    "\n",
    "\n",
    "fixed = {\n",
    "    \"num_classes\": 4,\n",
    "    \"n_fft\": 256,\n",
    "    \"hop_length\": 128,\n",
    "    \"kernel_size\": 3,\n",
    "    \"learning_rate\" : .001,\n",
    "}\n",
    "\n",
    "results = hyperparameter_search(\n",
    "    TCN_STFT_Classifier,\n",
    "    param_grid,\n",
    "    fixed,\n",
    "    device=device,\n",
    "    epochs=5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    augmented_data = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859870a",
   "metadata": {},
   "source": [
    "## We get our best parameter selection for the model trained with data augmentation for TCN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5028a46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toby_\\Documents\\TU_Berlin\\Semestre 3\\AMLS\\AMLS_packed\\venv\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.9650 - Train F1: 0.2736 - Val Loss: 0.8357 - Val F1: 0.4054\n",
      "Epoch 2/50 - Train Loss: 0.8224 - Train F1: 0.3944 - Val Loss: 0.7376 - Val F1: 0.4935\n",
      "Epoch 3/50 - Train Loss: 0.7503 - Train F1: 0.4549 - Val Loss: 0.6875 - Val F1: 0.4449\n",
      "Epoch 4/50 - Train Loss: 0.6898 - Train F1: 0.5139 - Val Loss: 0.6970 - Val F1: 0.5146\n",
      "Epoch 5/50 - Train Loss: 0.6438 - Train F1: 0.5835 - Val Loss: 0.6304 - Val F1: 0.5737\n",
      "Epoch 6/50 - Train Loss: 0.6245 - Train F1: 0.5828 - Val Loss: 0.6476 - Val F1: 0.4823\n",
      "Epoch 7/50 - Train Loss: 0.5934 - Train F1: 0.6272 - Val Loss: 0.5826 - Val F1: 0.6639\n",
      "Epoch 8/50 - Train Loss: 0.5788 - Train F1: 0.6337 - Val Loss: 0.6112 - Val F1: 0.6324\n",
      "Epoch 9/50 - Train Loss: 0.5585 - Train F1: 0.6468 - Val Loss: 0.5903 - Val F1: 0.6286\n",
      "Epoch 10/50 - Train Loss: 0.5346 - Train F1: 0.6806 - Val Loss: 0.5894 - Val F1: 0.6740\n",
      "Epoch 11/50 - Train Loss: 0.5312 - Train F1: 0.6820 - Val Loss: 0.5735 - Val F1: 0.6734\n",
      "Epoch 12/50 - Train Loss: 0.5180 - Train F1: 0.6884 - Val Loss: 0.5590 - Val F1: 0.7127\n",
      "Epoch 13/50 - Train Loss: 0.5032 - Train F1: 0.6848 - Val Loss: 0.5546 - Val F1: 0.7028\n",
      "Epoch 14/50 - Train Loss: 0.4955 - Train F1: 0.7035 - Val Loss: 0.5243 - Val F1: 0.6890\n",
      "Epoch 15/50 - Train Loss: 0.4848 - Train F1: 0.7126 - Val Loss: 0.5463 - Val F1: 0.6751\n",
      "Epoch 16/50 - Train Loss: 0.4866 - Train F1: 0.7198 - Val Loss: 0.5505 - Val F1: 0.7062\n",
      "Epoch 17/50 - Train Loss: 0.4698 - Train F1: 0.7300 - Val Loss: 0.5276 - Val F1: 0.6948\n",
      "Epoch 18/50 - Train Loss: 0.4673 - Train F1: 0.7290 - Val Loss: 0.6008 - Val F1: 0.6505\n",
      "Epoch 19/50 - Train Loss: 0.4624 - Train F1: 0.7409 - Val Loss: 0.5143 - Val F1: 0.7262\n",
      "Epoch 20/50 - Train Loss: 0.4484 - Train F1: 0.7533 - Val Loss: 0.5415 - Val F1: 0.7119\n",
      "Epoch 21/50 - Train Loss: 0.4449 - Train F1: 0.7468 - Val Loss: 0.5226 - Val F1: 0.7174\n",
      "Epoch 22/50 - Train Loss: 0.4447 - Train F1: 0.7441 - Val Loss: 0.5022 - Val F1: 0.7161\n",
      "Epoch 23/50 - Train Loss: 0.4256 - Train F1: 0.7537 - Val Loss: 0.5363 - Val F1: 0.7184\n",
      "Epoch 24/50 - Train Loss: 0.4253 - Train F1: 0.7654 - Val Loss: 0.4980 - Val F1: 0.7325\n",
      "Epoch 25/50 - Train Loss: 0.4199 - Train F1: 0.7798 - Val Loss: 0.5193 - Val F1: 0.7216\n",
      "Epoch 26/50 - Train Loss: 0.4210 - Train F1: 0.7713 - Val Loss: 0.4679 - Val F1: 0.7406\n",
      "Epoch 27/50 - Train Loss: 0.4169 - Train F1: 0.7732 - Val Loss: 0.5066 - Val F1: 0.7003\n",
      "Epoch 28/50 - Train Loss: 0.3955 - Train F1: 0.8001 - Val Loss: 0.4792 - Val F1: 0.7325\n",
      "Epoch 29/50 - Train Loss: 0.3884 - Train F1: 0.7882 - Val Loss: 0.5063 - Val F1: 0.7282\n",
      "Epoch 30/50 - Train Loss: 0.3935 - Train F1: 0.7773 - Val Loss: 0.5338 - Val F1: 0.7323\n",
      "Epoch 31/50 - Train Loss: 0.3825 - Train F1: 0.7920 - Val Loss: 0.5125 - Val F1: 0.7205\n",
      "\n",
      "Early stopping triggered at epoch 31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0     0.8529    0.9587    0.9027       363\n",
      "     class_1     0.8378    0.5741    0.6813        54\n",
      "     class_2     0.7817    0.6307    0.6981       176\n",
      "     class_3     0.5357    0.6818    0.6000        22\n",
      "\n",
      "    accuracy                         0.8211       615\n",
      "   macro avg     0.7520    0.7113    0.7205       615\n",
      "weighted avg     0.8199    0.8211    0.8139       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.models.model_2 import TCN_STFT_Classifier\n",
    "from src.models.model_trainer import Trainer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = TCN_STFT_Classifier(\n",
    "    num_classes=4,\n",
    "    hop_length = 128,\n",
    "    n_fft = 256,\n",
    "    kernel_size = 5, \n",
    "    hidden_channels=  [128, 128, 128, 128],\n",
    "    dropout = 0.1,\n",
    "    num_levels = 3,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, device=device, augment_data = True)\n",
    "\n",
    "history = trainer.fit(train_loader, val_loader, epochs=50)\n",
    "\n",
    "train_loss, train_f1 = trainer.evaluate(train_loader)\n",
    "\n",
    "val_loss, val_f1 = trainer.evaluate(val_loader)\n",
    "\n",
    "cm, report = trainer.detailed_metrics(val_loader, class_names=[\"class_0\", \"class_1\", \"class_2\", \"class_3\"])\n",
    "print(report)\n",
    "\n",
    "model.eval()  # Modo evaluación\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, lengths_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "            \n",
    "        outputs = model(X_batch, lengths_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)  # clase con mayor probabilidad\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "df = pd.DataFrame({'predicted_label': all_preds})\n",
    "    \n",
    "df.to_csv('augment.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
